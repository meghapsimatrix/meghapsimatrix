[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Megha Joshi","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["R"],"content":" Work in progress‚Ä¶\nIn this post, I walk through steps of running propensity score analysis when there is missingness in the covariate data. Particularly, I look at multiple imputation and ways to condition on propensity scores estimated with imputed data. The code builds on my earlier post where I go over different ways to handle missing data when conducting propensity score analysis.\nHill (2004) and Mitra and Reiter (2016) examined two distinct ways to condition on the propensity scores estimated on multiply imputed data:\nMultiple Imputation Across (MI Across) This approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit‚Äôs m propensity scores across the m datasets (Hill 2004). Stratification, matching or IPW can be implemented using these averaged propensity scores (Hill 2004). Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the m sets of covariate values. The weighted regression estimates will then need to be pooled.\n Multiple Imputation Within (MI Within) This approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets (Hill 2004). Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset (Hill 2004). The separate regression estimates have to be pooled.\n Read in the data Below, I show how to implement the Across and Within methods to estimate the average treatment effect on the treated (ATT). The data that I use here is from High School and Beyond (HSB) longitudinal study used by Rosenbaum (1986) to analyze the effect of dropping out of high school on later math achievement for students who dropped out. For the purpose of this example, I am going to assume a simple random sample.\nlibrary(tidyverse) library(mice) library(twang) library(estimatr) library(broom) library(naniar) library(knitr) hsls_dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/meghapsimatrix/datasets/master/causal/HSLS09_incomplete.csv\u0026quot;) %\u0026gt;% mutate_if(is.character, as.factor) %\u0026gt;% mutate_at(vars(repeated_grade, IEP), as.factor) %\u0026gt;% select(-working_T3) # an outcome we don\u0026#39;t care about Below, using gg_miss_var from the naniar package, I visualize the percent of data that are missing for each of the variables (Tierney et al. 2020). The treatment variable, drop_status has no missing data. The outcome variable, math_score_T2, however, does have around 10% missing data. I am going to impute the outcome data and the covariate data in this example. Please see Hill (2004) for a discussion on dealing with missing outcome data.\ngg_miss_var(hsls_dat, show_pct = TRUE)  Multiple imputation using mice Here, I impute the data using the mice package and set the number of imputations and iterations to 10 each (van Buuren and Groothuis-Oudshoorn 2011). Then, I save the result as an RData file that I can load later. For the imputation method, I let mice run the default methods: predicitive mean matching for continuous variables, Bayesian logistic regression for binary variables and Bayesian polytomous regression for multinomial variables. For more information on different methods for imputation, please see Chapter 6.2 from Flexible Modeling of Missing Data (van Buuren 2018).\nsystem.time(temp_data \u0026lt;- mice(hsls_dat, m = 10, maxit = 10, seed = 20200516)) save(temp_data, file = \u0026quot;temp_data.RData\u0026quot;) Imputed data I load the saved RData file and then extract the data that contains each of the 10 imputed data stacked.\nload(\u0026quot;temp_data.RData\u0026quot;) imp_dat \u0026lt;- complete(temp_data, action = \u0026quot;long\u0026quot;)   Estimating propensity scores Below I create a dataset with only the covariates and use the paste() function to create a propensity score model equation. For the sake of this example, I only focus on including main effects of the covariates in the propensity score model.\ncovs \u0026lt;- imp_dat %\u0026gt;% select(sex:climate, math_score_T1) equation_ps \u0026lt;- paste(\u0026quot;drop_status ~ \u0026quot;, paste(names(covs), collapse = \u0026quot; + \u0026quot;)) equation_ps ## [1] \u0026quot;drop_status ~ sex + race + language + repeated_grade + IEP + locale + region + SES + math_identity + math_utility + math_efficacy + math_interest + engagement + belonging + expectations + climate + math_score_T1\u0026quot; Here, I create a function estimate_ps() that takes in an equation and a dataset and runs logistic regression using glm(). The function then adds the logit of propensity scores and the propensity scores as columns in the data.\nI then group the imp_dat by the imputation number and then run the estimate_ps() function on each of the imputed dataset using the do() function from dplyr (Wickham et al. 2019).\nestimate_ps \u0026lt;- function(equation, dat){ ps_model \u0026lt;- glm(as.formula(equation), family = binomial, data = dat) dat \u0026lt;- dat %\u0026gt;% mutate(ps_logit = predict(ps_model, type = \u0026quot;link\u0026quot;), ps = predict(ps_model, type = \u0026quot;response\u0026quot;)) return(dat) } imp_dat_ps \u0026lt;- imp_dat %\u0026gt;% group_by(.imp) %\u0026gt;% do(estimate_ps(equation_ps, .)) %\u0026gt;% ungroup()  Estimating ATT IPW weights Below I estimate ATT weights using the across and within methods. For the Across method, I use the average of the propensity scores across the imputed datasets to calculate weights. For the within method, I use the the propensity scores estimated within each imputed dataset to calculate weights.\nThe code below groups the imputed data by .id which is an identifier denoting each case. For each case, I summarize the mean of the propensity scores across the 10 imputed dataset and add that mean propensity score as a variable ps_across in the data. Then, I estimate the ATT weights using the averaged propensity scores for the Across method and the original propensity scores for the Within method.\nimp_dat_ps \u0026lt;- imp_dat_ps %\u0026gt;% group_by(.id) %\u0026gt;% mutate(ps_across = mean(ps)) %\u0026gt;% ungroup() %\u0026gt;% mutate(att_wt_across = drop_status + (1 - drop_status) * ps_across/(1 - ps_across), att_wt_within = drop_status + (1 - drop_status) * ps/(1 - ps)) imp_dat_ps %\u0026gt;% select(.imp, ps, ps_across, att_wt_across, att_wt_within) ## # A tibble: 214,020 x 5 ## .imp ps ps_across att_wt_across att_wt_within ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.00339 0.00330 0.00331 0.00340 ## 2 1 0.00989 0.0102 0.0103 0.00999 ## 3 1 0.00135 0.00131 0.00132 0.00135 ## 4 1 0.0119 0.0114 0.0115 0.0120 ## 5 1 0.00222 0.00246 0.00246 0.00222 ## 6 1 0.00289 0.00294 0.00295 0.00290 ## 7 1 0.0113 0.0101 0.0102 0.0114 ## 8 1 0.00347 0.00375 0.00377 0.00348 ## 9 1 0.00327 0.00276 0.00276 0.00328 ## 10 1 0.000667 0.000633 0.000633 0.000667 ## # ‚Ä¶ with 214,010 more rows  Checking common support Across The propensity scores are averaged across the imputations when using the Across method. Thus, I create a density plot showing the distribution of the logit of the propensity scores from one of the imputations (all of the imputations will have the same distribution). The distribution of the propensity scores for drop-outs overlaps with that for the stayers satisfying the common support assumption.\nimp_dat_ps %\u0026gt;% mutate(drop = if_else(drop_status == 1, \u0026quot;Drop-outs\u0026quot;, \u0026quot;Stayers\u0026quot;), ps_across_logit = log(ps_across/ (1 - ps_across))) %\u0026gt;% filter(.imp == 1) %\u0026gt;% ggplot(aes(x = ps_across_logit, fill = drop)) + geom_density(alpha = .5) + labs(x = \u0026quot;Logit Propensity Scores\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;\u0026quot;) + ggtitle(\u0026quot;Common Support: Across Method\u0026quot;) + theme_bw()  Within The propensity scores estimated for each imputation are used when using the Within method. Below, I create density plots showing distributions of the logit of the propensity scores faceted by the imputation. The common support assumption seems to be satisfied across all the imputations.\nIn other datasets, the common support assumption may be violated. In such a case, certain cases might need to be trimmed from the analysis so there is enough overlap of the distributions.\nimp_dat_ps %\u0026gt;% mutate(drop = if_else(drop_status == 1, \u0026quot;Drop-outs\u0026quot;, \u0026quot;Stayers\u0026quot;)) %\u0026gt;% ggplot(aes(x = ps_logit, fill = drop)) + geom_density(alpha = .5) + labs(x = \u0026quot;Logit Propensity Scores\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;\u0026quot;) + ggtitle(\u0026quot;Common Support: Within Method\u0026quot;) + facet_wrap(~ .imp, ncol = 2) + theme_bw()   Checking balance  Estimating ATT Below I create a function to estimate the ATT. The function takes in an equation, a dataset, and weights as arguments. Then, it runs a model using lm_robust() from the estimatr package (Blair et al. 2020). The standard errors of the regression coefficients are estimated using HC2 type sandwich errors. It then cleans up the results using tidy() from broom (Robinson and Hayes 2020).\nestimate_ATT \u0026lt;- function(equation, dat, wts){ wts \u0026lt;- dat %\u0026gt;% pull({{wts}}) model \u0026lt;- lm_robust(as.formula(equation), data = dat, weights = wts) res \u0026lt;- model %\u0026gt;% tidy() %\u0026gt;% filter(term == \u0026quot;drop_status\u0026quot;) %\u0026gt;% select(term, estimate, se = std.error, dci_low = conf.low, ci_high = conf.high, df = df) return(res) } I set up an equation regressing the outcome variable, math_score_T2 on drop status and on the main effects of all the covariates. Then, I run the estimate_ATT() function on each imputed data using group_by() and do(). Note that the weights are different for the Across and Within methods.\nequation_ancova \u0026lt;- paste(\u0026quot;math_score_T2 ~ drop_status + \u0026quot;, paste(names(covs), collapse = \u0026quot; + \u0026quot;)) equation_ancova ## [1] \u0026quot;math_score_T2 ~ drop_status + sex + race + language + repeated_grade + IEP + locale + region + SES + math_identity + math_utility + math_efficacy + math_interest + engagement + belonging + expectations + climate + math_score_T1\u0026quot; across_res \u0026lt;- imp_dat_ps %\u0026gt;% group_by(.imp) %\u0026gt;% do(estimate_ATT(equation = equation_ancova, dat = ., wts = att_wt_across)) %\u0026gt;% ungroup() within_res \u0026lt;- imp_dat_ps %\u0026gt;% group_by(.imp) %\u0026gt;% do(estimate_ATT(equation = equation_ancova, dat = ., wts = att_wt_within)) %\u0026gt;% ungroup()  Pooling the results Here, I create a function called calc_pooled() using formula by Barnard and Rubin (1999) to pool the results across the imputations. The mice package has the pool() function to do the same thing but we would need to convert the imputed data back to mids object type and I just wanted to skip all that :D\ncalc_pooled \u0026lt;- function(dat, est, se, df){ dat \u0026lt;- dat %\u0026gt;% mutate(est = dat %\u0026gt;% pull({{est}}), se = dat %\u0026gt;%pull({{se}}), df = dat %\u0026gt;% pull({{df}})) pooled \u0026lt;- dat %\u0026gt;% summarize(m = n(), B = var(est), # between imputation var beta_bar = mean(est), # mean of estimated reg coeffs V_bar = mean(se^2), # mean of var - hc corrected within imp var eta_bar = mean(df)) %\u0026gt;% # mean of df mutate( V_total = V_bar + B * (m + 1) / m, #between and within var est gamma = ((m + 1) / m) * B / V_total, df_m = (m - 1) / gamma^2, df_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3), df = 1 / (1 / df_m + 1 / df_obs), # output se = sqrt(V_total), ci_lower = beta_bar - se * qt(0.975, df = df), ci_upper = beta_bar + se * qt(0.975, df = df)) %\u0026gt;% select(est = beta_bar, se, df, ci_lower, ci_upper) return(pooled) } Below I use the calc_pooled() function to pool the results for each of the methods.\nAcross across_pooled \u0026lt;- calc_pooled(dat = across_res, est = estimate, se = se, df = df) across_pooled %\u0026gt;% kable(digits = 3)   est se df ci_lower ci_upper    -0.335 0.038 93.772 -0.411 -0.26     Within within_pooled \u0026lt;- calc_pooled(dat = within_res, est = estimate, se = se, df = df) within_pooled %\u0026gt;% kable(digits = 3)   est se df ci_lower ci_upper    -0.356 0.041 50.002 -0.438 -0.273     Interpretation Across For students who dropped out, if they drop out of high school, they are expected to score -0.335, 95% CI[-0.411, -0.26] lower on math score in 2012 compared to if they stayed.\n Within For students who dropped out, if they drop out of high school, they are expected to score -0.356, 95% CI[-0.438, -0.273] lower on math score in 2012 compared to if they stayed.\n   Comparing the methods The estimates, se and df are different for the results from the two different methods. So which one is better? It‚Äôs not possible to say based on analysis on one dataset. Hill (2004) and Mitra and Reiter (2016) conducted simulation studies comparing the two methods. Hill (2004) found that MI Across performed best in terms of absolute bias and mean squared error compared to all the other methods examined in the study. Mitra and Reiter (2016) found that MI Across resulted in greater bias reduction in the estimation of ATT compared to MI Within. For details of the simulation studies please see the articles.\n References Barnard, John, and Donald B Rubin. 1999. ‚ÄúSmall-Sample Degrees of Freedom with Multiple Imputation,‚Äù 9.\n Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2020. Estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n Hill, Jennifer. 2004. ‚ÄúReducing Bias in Treatment Effect Estimation in Observational Studies Suffering from Missing Data.‚Äù Columbia University Institute for Social \u0026amp; Economic Research \u0026amp; Policy (ISERP).\n Mitra, Robin, and Jerome P Reiter. 2016. ‚ÄúA Comparison of Two Methods of Estimating Propensity Scores After Multiple Imputation.‚Äù Statistical Methods in Medical Research 25 (1): 188‚Äì204. https://doi.org/10.1177/0962280212445945.\n Robinson, David, and Alex Hayes. 2020. Broom: Convert Statistical Analysis Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n Rosenbaum, Paul R. 1986. ‚ÄúDropping Out of High School in the United States: An Observational Study.‚Äù Journal of Educational Statistics 11 (3). Sage Publications Sage CA: Los Angeles, CA: 207‚Äì24.\n Tierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2020. Naniar: Data Structures, Summaries, and Visualisations for Missing Data. https://CRAN.R-project.org/package=naniar.\n van Buuren, Stef. 2018. Flexible Imputation of Missing Data. Chapman; Hall/CRC.\n van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. ‚Äúmice: Multivariate Imputation by Chained Equations in R.‚Äù Journal of Statistical Software 45 (3): 1‚Äì67. http://www.jstatsoft.org/v45/i03/.\n Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the tidyverse.‚Äù Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n   ","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"09bda55c00691e9b0be420432204f95a","permalink":"/post/mi_ps/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/post/mi_ps/","section":"post","summary":"Work in progress‚Ä¶\nIn this post, I walk through steps of running propensity score analysis when there is missingness in the covariate data. Particularly, I look at multiple imputation and ways to condition on propensity scores estimated with imputed data. The code builds on my earlier post where I go over different ways to handle missing data when conducting propensity score analysis.\nHill (2004) and Mitra and Reiter (2016) examined two distinct ways to condition on the propensity scores estimated on multiply imputed data:","tags":["propensity scores","missing data","multiple imputation","causal"],"title":"Propensity Score Analysis with Multiply Imputed Data","type":"post"},{"authors":null,"categories":["R"],"content":" In this post I am visualizing and analyzing the unprecedented increase in the number of unemployment claims filed in the US after the lockdown due to COVID 19 pandemic. I am retrieving the data from the tidyquant package (Dancho \u0026amp; Vaughan, 2020).\nlibrary(CausalImpact) library(tidyverse) library(scales) library(tidyquant) ICSA Data Initial unemployment claims from the first date available, 1967:\nicsa_dat \u0026lt;- \u0026quot;ICSA\u0026quot; %\u0026gt;% tq_get(get = \u0026quot;economic.data\u0026quot;, from = \u0026quot;1967-01-07\u0026quot;) %\u0026gt;% rename(claims = price) glimpse(icsa_dat) ## Rows: 2,789 ## Columns: 3 ## $ symbol \u0026lt;chr\u0026gt; \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;‚Ä¶ ## $ date \u0026lt;date\u0026gt; 1967-01-07, 1967-01-14, 1967-01-21, 1967-01-28, 1967-02-04, 1‚Ä¶ ## $ claims \u0026lt;int\u0026gt; 208000, 207000, 217000, 204000, 216000, 229000, 229000, 242000‚Ä¶ icsa_dat %\u0026gt;% ggplot(aes(x = date, y = claims)) + geom_line(color = \u0026quot;blue\u0026quot;) + scale_y_continuous(labels = comma) + labs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Claims\u0026quot;, subtitle = \u0026quot;As of June 21, 2020\u0026quot;) + ggtitle(\u0026quot;Unemployment Claims: 1967 to 2020\u0026quot;) + theme_bw()  Comparison to 2008 Recession In the graph below, I only selected 2008 to 2020. We can compare the unemployment claims during the 2008 recession to the number of claims filed during the COVID-19 lockdown. What is happening now is preposterous.\nicsa_dat %\u0026gt;% mutate(year = year(date)) %\u0026gt;% filter(year \u0026gt; 2007) %\u0026gt;% ggplot(aes(x = date, y = claims)) + geom_line(color = \u0026quot;blue\u0026quot;) + scale_y_continuous(labels = comma) + labs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Claims\u0026quot;, subtitle = \u0026quot;As of June 21, 2020\u0026quot;) + ggtitle(\u0026quot;Unemployment Claims: 2008 to 2020\u0026quot;) + theme_bw()  Causal Inference Sometimes #causalinference is simple.\n‚ÄúWhat's the immediate causal effect of the #COVID19 lockdowns on unemployment?‚Äù\nThe answer is ‚ÄúUnprecedented‚Äù. We know we're in deep trouble when a time series is all we need. https://t.co/cXK0wLw3no pic.twitter.com/kS4PvVwihM ‚Äî Miguel Hern√°n (@_MiguelHernan) March 29, 2020   Below, I use the CausalImpact package to run a Bayesian structural time-series analysis (Brodersen et al., 2015). For more information on the package, please see this vignette. Typically, it would be good to add covariates in the analysis but the data does not have any and given the rate of increase, I highly doubt that the inclusion of covariates would matter much. It would be interesting to compare the number of claims filed in US versus the number of claims filed in country with better social and economic security systems in place (perhaps the Netherlands). The impact of COVID-19 lockdowns on the number of unemployment claims is probably exacerbated by the lack of social and economic security in the US. In addition, due to employer based healthcare system in the US, millions of people have lost or are going to lose health insurance. Now more than every we need Medicare for All, $2000 a month stimulus, Green New Deal. The impact of climate change will be worse.\nProjected‚¨ÜÔ∏èin unemployment:\nüá©üá™: 3.2%‚û°Ô∏è5.9%\nüá¨üáß: 3.9%‚û°Ô∏è7%\nüá´üá∑: 8.5%‚û°Ô∏è12%\nüá∫üá∏: 3.5%‚û°Ô∏è32.1%\nProjected‚¨ÜÔ∏è in # of uninsured:\nüá©üá™: 0\nüá¨üáß: 0\nüá´üá∑: 0\nüá∫üá∏: At least 12 million\nSolution: Guarantee healthcare and paychecks like other wealthy countries do. https://t.co/44ijS2evzL ‚Äî Warren Gunnels (@GunnelsWarren) April 21, 2020   dates \u0026lt;- icsa_dat %\u0026gt;% pull(date) # create pre and post pre_period \u0026lt;- c(dates[1], dates[2776]) post_period \u0026lt;- c(dates[2777], dates[length(dates)]) # make into dat dat \u0026lt;- icsa_dat %\u0026gt;% select(date, y = claims) # causal impact impact \u0026lt;- CausalImpact(dat, pre_period, post_period) sum_impact \u0026lt;- impact$summary %\u0026gt;% mutate(type = rownames(.)) %\u0026gt;% pivot_longer(cols = -type, names_to = \u0026quot;stats\u0026quot;, values_to = \u0026quot;vals\u0026quot;) avg_impact \u0026lt;- sum_impact %\u0026gt;% mutate(vals = round(vals/1000000, 2)) rel_impact \u0026lt;- sum_impact %\u0026gt;% filter(str_detect(stats, \u0026quot;Rel\u0026quot;)) %\u0026gt;% mutate(vals = round(vals * 100)) # summary(impact, \u0026quot;report\u0026quot;)   Analysis report: CausalImpact Below is the report generated by CausalImpact with some edits by me.\nSumming up the individual data points during the post-lockdown period, the total number of unemployment claims filed equaled 45.74M. By contrast, had the intervention not taken place, we would have expected a sum of 3.28M. The 95% interval of this prediction is [2.7M, 3.84M].\nThe probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.\n References Brodersen et al., 2015, Annals of Applied Statistics. Inferring causal impact using Bayesian structural time-series models. http://research.google.com/pubs/pub41854.html\nDancho, M. and Vaughan, D. (2020). tidyquant: Tidy Quantitative Financial Analysis. R package version 1.0.0. https://CRAN.R-project.org/package=tidyquant\nWickham, H. and Seidel, D. (2019). scales: Scale Functions for Visualization. R package version 1.1.0. https://CRAN.R-project.org/package=scales\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\n ","date":1592697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592697600,"objectID":"4963767a9f89f468ccdbd762c6ec13e5","permalink":"/post/unemployment_claims/","publishdate":"2020-06-21T00:00:00Z","relpermalink":"/post/unemployment_claims/","section":"post","summary":"In this post I am visualizing and analyzing the unprecedented increase in the number of unemployment claims filed in the US after the lockdown due to COVID 19 pandemic. I am retrieving the data from the tidyquant package (Dancho \u0026amp; Vaughan, 2020).\nlibrary(CausalImpact) library(tidyverse) library(scales) library(tidyquant) ICSA Data Initial unemployment claims from the first date available, 1967:\nicsa_dat \u0026lt;- \u0026quot;ICSA\u0026quot; %\u0026gt;% tq_get(get = \u0026quot;economic.data\u0026quot;, from = \u0026quot;1967-01-07\u0026quot;) %\u0026gt;% rename(claims = price) glimpse(icsa_dat) ## Rows: 2,789 ## Columns: 3 ## $ symbol \u0026lt;chr\u0026gt; \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;, \u0026quot;ICSA\u0026quot;‚Ä¶ ## $ date \u0026lt;date\u0026gt; 1967-01-07, 1967-01-14, 1967-01-21, 1967-01-28, 1967-02-04, 1‚Ä¶ ## $ claims \u0026lt;int\u0026gt; 208000, 207000, 217000, 204000, 216000, 229000, 229000, 242000‚Ä¶ icsa_dat %\u0026gt;% ggplot(aes(x = date, y = claims)) + geom_line(color = \u0026quot;blue\u0026quot;) + scale_y_continuous(labels = comma) + labs(x = \u0026quot;Date\u0026quot;, y = \u0026quot;Claims\u0026quot;, subtitle = \u0026quot;As of June 21, 2020\u0026quot;) + ggtitle(\u0026quot;Unemployment Claims: 1967 to 2020\u0026quot;) + theme_bw()  Comparison to 2008 Recession In the graph below, I only selected 2008 to 2020.","tags":["coronavirus","COVID-19","lockdown","ggplot","unemployment claims","time-series","causal"],"title":"Unemployment Claims COVID-19","type":"post"},{"authors":null,"categories":["R"],"content":" Theories behind propensity score analysis assume that the covariates are fully observed (Rosenbaum \u0026amp; Rubin, 1983, 1984). However, in practice, observational analyses require large administrative databases or surveys, which inevitably will have missingness in the covariates. The response patterns of people with missing covariates may be different than those of people with observed data (Mohan, Pearl, \u0026amp; Tian, 2013). Therefore, ways to handle missing covariate data need to be examined. The basic estimation of propensity scores using logistic regression will delete cases with missing data, which can be problematic as it can cause bias in the treatment effect estimates (Baraldi \u0026amp; Enders, 2010).\nMissing Data Methods in Propensity Score Analysis Below I explain three major methods used in the applied propensity score analysis literature when \\(X\\) is not fully observed. I also explain three other methods to handle missing data that are not commonly used in applied literature but have been proposed theoretically. I also describe the assumptions about missing data and strong ignorability underlying each of the methods. Let \\(X_{obs}\\) indicate the observed parts of \\(X\\) and \\(X_{mis}\\) indicate the missing parts of \\(X\\). \\(D\\) indicates the fully observed treatment indicator and \\(Y\\) indicates a fully observed outcome variable.\nComplete Case Analysis This approach deletes cases with missing data in any of the variables used in the analysis (Baraldi \u0026amp; Enders, 2010; Hill, 2004). The traditional propensity score estimation method of using logistic regression implements complete case analysis by default. Therefore, this method is commonly used in applied research. The data that remains after deleting cases with missing data are assumed to be a simple random sample of the full data (Baraldi \u0026amp; Enders, 2010). Missingness is not related to any study variables nor to the hypothetically complete values of itself (Equations and ). According to Hill (2004), the assumption underlying complete case analysis is that the joint distributions of \\(X_{obs}\\) and \\(X_{mis}\\) are same across the two treatment conditions: \\[\\begin{equation} X_{obs}, X_{mis} \\perp\\!\\!\\!\\perp D \\end{equation}\\] Therefore, an unbiased causal effect estimate can be retrieved after deleting cases with missing data. Such an assumption is very stringent and unlikely to be met in the types of data required for propensity score analyses (Baraldi \u0026amp; Enders, 2010; Hill, 2004). As mentioned above, deleting cases can also result in loss of power. Additionally, whether \\(X_{mis}\\) is balanced between the treatment groups cannot be confirmed.\n Multiple Imputation Multiple imputation (MI) generates multiple sets of data with the missing values drawn from an imputation model (Mitra \u0026amp; Reiter, 2016; Rubin, 1987). MI will create \\(m \u0026gt; 1\\) imputed datasets that contain different imputed values (Murray, 2018; van Buuren, 2018). Analyses can be performed on each of the datasets and results from each dataset can be aggregated across to derive a final estimate, standard error, degrees of freedom, and test result. Thus, MI involves two stages: (1) imputation and creation of the \\(m\\) imputed datasets, and (2) analysis and pooling of estimates across the datasets (Murray, 2018; van Buuren, 2018).\nThere are two approaches for imputing multivariate missing data: (1) joint modeling, JM, and (2) fully conditional specification, FCS, also called multivariate imputation by chained equations, MICE (Murray, 2018; van Buuren, 2018; van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). JM entails jointly modeling variables with missingness by drawing from a multivariate distribution (Murray, 2018; van Buuren, 2018; van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). FCS entails univariate conditional imputation models of variables with missing data that iteratively condition on all other variables using Monte Carlo Markov chain methods (van Buuren, 2018; van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). JM imputes all variables simultaneously whereas FCS imputes one variable at a time (van Buuren, 2018). Because JM requires specification of a joint distribution for all the variables, it may not be as flexible as FCS when dealing with a large number of covariates with missing data (Akande, Li, \u0026amp; Reiter, 2017). However, FCS is computationally more intensive than JM (van Buuren, 2018). FCS also has been shown to outperform JM for categorical variables and is more robust under mis-specification of imputation model (van Buuren, 2018). Therefore, van Buuren (2018) recommended to use FCS over JM.\nIf the missingness mechanism is MAR or MCAR and if assumptions underlying the imputation model are correct, MI will yield unbiased results, as it uses the information available in \\(X_{obs}\\) to impute missing values (Murray, 2018). In the causal inference context, Hill (2004) argued that MI relies on the assumption of latent ignorability, a concept introduced by Frangakis \u0026amp; Rubin (1999). The assumption requires that the treatment assignment mechanism is ignorable given complete covariate data including the values that are latent or missing. These missing values are derived from MI. Below, let \\(e_{MI}(X)\\) denote propensity scores derived after multiple imputation: \\[\\begin{equation} X_{obs}, X_{mis} \\perp\\!\\!\\!\\perp D| e_{MI}(X) \\end{equation}\\] \\[\\begin{equation} Y(1), Y(0) \\perp\\!\\!\\!\\perp D | e_{MI}(X) \\end{equation}\\] Hill (2004) proposed two different ways to combine propensity scores estimated in each of the m datasets:\nMultiple Imputation Across (MI Across) This approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit‚Äôs m propensity scores across the m datasets (Hill, 2004). Stratification, matching or IPW can be implemented using these averaged propensity scores (Hill, 2004). Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the m sets of covariate values. The weighted regression estimates will then need to be pooled.\n Multiple Imputation Within (MI Within) This approach involves creating m imputed datasets and then estimating propensity scores within each of the datasets (Hill, 2004). Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset (Hill, 2004). The separate regression estimates have to be pooled.\n  Generalized Propensity Scores Rosenbaum \u0026amp; Rubin (1984) proposed the use of generalized propensity scores (GPS) as a way to address missing covariate data. The GPS represents the probability of treatment given observed covariates and missingness indicators (Rosenbaum \u0026amp; Rubin, 1984): \\[\\begin{equation} e^*(X) = P(D = 1|X_{obs}, R) \\end{equation}\\] Conditioning on \\(e^*(X)\\) will balance the treatment groups in terms of the observed covariates and missingness patterns (Rosenbaum \u0026amp; Rubin, 1984). The observed part of \\(X\\) and the missingness pattern indicators, \\(R\\), will be independent of treatment assignment given the GPS (Rosenbaum \u0026amp; Rubin, 1984): \\[\\begin{equation} X_{obs}, R \\perp\\!\\!\\!\\perp D| e^*(X) \\end{equation}\\] However, conditioning on GPS will not balance the groups in terms of the unobserved values of \\(X\\) (Rosenbaum \u0026amp; Rubin, 1984): \\[\\begin{equation} X_{mis} \\not\\!\\perp\\!\\!\\!\\perp D| e^*(X) \\end{equation}\\] Although this technique of treating missing data is not generally recommended for other types of missing data analyses, it has been recommended for use in propensity score analysis literature (Rosenbaum \u0026amp; Rubin, 1984; Stuart, 2010). In the context of propensity score analysis, this approach does not assume latent ignorability of treatment assignment because legitimate values for missing data are never derived. The assumption underlying this method is that balancing the treatment and control groups on \\(X_{obs}\\) and \\(R\\) is a sufficient condition to satisfy ignorability. With the GPS, the treatment and control groups are possibly not going to be balanced in terms of \\(X_{mis}\\).\nFor large studies with few missing data patterns, Rosenbaum \u0026amp; Rubin (1984) suggested estimating separate logit models for each missingness pattern. In practice, it is common to encounter many patterns of missing data. For these scenarios, Rosenbaum \u0026amp; Rubin (1984) suggested creating an additional category indicating missingness for categorical variables. For continuous variables, Stuart (2010) recommended imputing missing data with a single arbitrary value, such as the overall mean of the covariate, and then creating a missingness indicator variable. In general missing data analysis context, van Buuren (2018) noted that this method of combining arbitrary (mean) imputation along with missingness indicators can underestimate the standard error of the estimate of interest.\nThe CART algorithms treat missing data natively as they split missingness as a category itself. In this manner, this approach is similar to the GPS which uses missingness pattern indicators when estimating propensity scores. The missingness categories are used to estimate propensity scores and conditioning on the propensity scores should balance the treatment and control condition in terms of the patterns. However, splitting does not actually impute the missing data so it is plausible to assume that like GPS, scores derived using the splitting method will not balance the groups in terms of the latent missing data. In addition, unlike MI, there are no imputed complete datasets saved to analyze for the outcome model. Therefore, splitting would need to be combined with some other technique for outcome modeling.\n Other Methods The following methods have been discussed theoretically in literature examining missing data methods in propensity score analysis. However, these methods are not commonly used in applied literature.\nComplete Variables This method removes any variable with missing data (Hill, 2004). By removing variables with missing data, the approach assumes that the distribution of those variables (both the observed and missing parts) are the same across the two treatment groups (Hill, 2004). If this assumption does not hold, then this method can result in bias in treatment effect estimates due to removal of important confounding variables (Hill, 2004).\n D‚ÄôAgostino and Rubin Expectation Maximization Another approach is a method introduced by D‚ÄôAgostino \u0026amp; Rubin (2000), which estimates propensity scores using an Expectation Conditional Maximization (ECM) algorithm (Hill, 2004). This method, DR, works similar to GPS as it models \\(X_{obs}\\), \\(R\\), and the treatment indicator variable. However, instead of imputing \\(X_{mis}\\), the DR method uses ECM to estimate propensity scores in presence of missing data (Hill, 2004). The assumption underlying DR is that within each missingness pattern defined by \\(R\\), \\(X_{mis}\\) is independent of \\(D\\) given the observed data, \\(X_{obs}\\) (Hill, 2004): \\[\\begin{equation} X_{mis} \\perp\\!\\!\\!\\perp D| X_{obs}, R \\end{equation}\\] Such independence is sufficient to satisfy the ignorability assumption in presence of missing covariate data. With this method, the assumption cannot be checked, however, as DR does not actually impute the missing values. This method is not readily available in commonly used software like R.\n Multiple Imputation Missingness Indicator Pattern Mixutre Qu \u0026amp; Lipkovich (2009) extended MI by introducing the missingness indicator pattern mixture (MIMP) approach, which is the same as MI but adds \\(R\\) in the propensity score estimation model. The rationale behind this approach is to use information given by missingness patterns to estimate treatment propensities. The method will assume latent ignorabilty. However, this approach should also balance the treatment group on \\(R\\) as \\(R\\) is used to estimate the propensity scores: \\[\\begin{equation} X_{obs}, X_{mis}, R \\perp\\!\\!\\!\\perp D| e_{MIMP}(X) \\end{equation}\\] Qu \u0026amp; Lipkovich (2009) argued that extending MI by adding R to the propensity score estimation accounts for non-ignorability or MNAR (Qu \u0026amp; Lipkovich, 2009; van Buuren, 2018). This method allows missingness itself to provide information on missingness: \\[\\begin{equation} P(X| X_{obs}, R = 1) \\neq P(X| X_{obs}, R = 0) \\end{equation}\\]\n   References Akande, O., Li, F., \u0026amp; Reiter, J. (2017). An empirical comparison of multiple imputation methods for categorical data. The American Statistician, 71(2), 162‚Äì170.\n Baraldi, A. N., \u0026amp; Enders, C. K. (2010). An introduction to modern missing data analyses. Journal of School Psychology, 48(1), 5‚Äì37. https://doi.org/10.1016/j.jsp.2009.10.001\n D‚ÄôAgostino, R. B., \u0026amp; Rubin, D. B. (2000). Estimating and Using Propensity Scores with Partially Missing Data. Journal of the American Statistical Association, 95(451), 749. https://doi.org/10.2307/2669455\n Frangakis, C., \u0026amp; Rubin, D. B. (1999). Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes. Biometrika, 86(2), 365‚Äì379. https://doi.org/10.1093/biomet/86.2.365\n Hill, J. (2004). Reducing bias in treatment effect estimation in observational studies suffering from missing data. Columbia University Institute for Social \u0026amp; Economic Research \u0026amp; Policy (ISERP).\n Mitra, R., \u0026amp; Reiter, J. P. (2016). A comparison of two methods of estimating propensity scores after multiple imputation. Statistical Methods in Medical Research, 25(1), 188‚Äì204. https://doi.org/10.1177/0962280212445945\n Mohan, K., Pearl, J., \u0026amp; Tian, J. (2013). Graphical models for inference with missing data. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, \u0026amp; K. Q. Weinberger (Eds.), Advances in neural information processing system (pp. 1277‚Äì1285). Red Hook, NY: Curran Associates, Inc.\n Murray, J. S. (2018). Multiple Imputation: A Review of Practical and Theoretical Findings. Statistical Science, 33(2), 142‚Äì159. https://doi.org/10.1214/18-STS644\n Qu, Y., \u0026amp; Lipkovich, I. (2009). Propensity score estimation with missing values using a multiple imputation missingness pattern (MIMP) approach. Statistics in Medicine, 28(9), 1402‚Äì1414. https://doi.org/10.1002/sim.3549\n Rosenbaum, P. R., \u0026amp; Rubin, D. B. (1983). The Central Role of the Propensity Score in Observational Studies for Causal Effects. Biometrika, 70(1), 41. https://doi.org/10.2307/2335942\n Rosenbaum, P. R., \u0026amp; Rubin, D. B. (1984). Reducing Bias in Observational Studies Using Subclassification on the Propensity Score. Journal of the American Statistical Association, 79(387), 516. https://doi.org/10.2307/2288398\n Rubin, D. B. (1987). Multiple imputation for nonresponse in surveys. New York: Wiley.\n Stuart, E. A. (2010). Matching Methods for Causal Inference: A Review and a Look Forward. Statistical Science, 25(1), 1‚Äì21. https://doi.org/10.1214/09-STS313\n van Buuren, S. (2018). Flexible imputation of missing data. Chapman; Hall/CRC.\n van Buuren, S., \u0026amp; Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in r. Journal of Statistical Software, 45(3), 1‚Äì67. Retrieved from http://www.jstatsoft.org/v45/i03/\n   ","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586995200,"objectID":"032142676fa4f11cdb208c2bf55e511d","permalink":"/post/missing_dat/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/post/missing_dat/","section":"post","summary":"Theories behind propensity score analysis assume that the covariates are fully observed (Rosenbaum \u0026amp; Rubin, 1983, 1984). However, in practice, observational analyses require large administrative databases or surveys, which inevitably will have missingness in the covariates. The response patterns of people with missing covariates may be different than those of people with observed data (Mohan, Pearl, \u0026amp; Tian, 2013). Therefore, ways to handle missing covariate data need to be examined.","tags":["propensity score","missing data","causal inference"],"title":"Missing Data in Propensity Score Analysis","type":"post"},{"authors":null,"categories":["Project"],"content":" Here is the website for the package.\nMonte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions (Morris, White, \u0026amp; Crowther, 2019). Methodologists use simulations to examine questions such as: (1) how does ordinary least squares regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? (3) how does cluster robust variance estimation perform when the number of clusters is small? To answer such questions, we conduct experiments by simulating thousands of datasets based on pseudo-random sampling, applying statistical methods, and evaluating how well those statistical methods recover the true data-generating conditions (Morris et al., 2019).\nThe goal of simhelpers is to assist in running simulation studies. The main tools in the package consist of functions to calculate measures of estimator performance like bias, root mean squared error, rejection rates. The functions also calculate the associated Monte Carlo standard errors (MCSE) of the performance measures. These functions are divided into three major categories of performance criteria: absolute criteria, relative criteria, and criteria to evaluate hypothesis testing. The functions use the tidyeval principles, so that they play well with dplyr and fit easily into a %\u0026gt;%-centric workflow (Wickham et al., 2019).\nIn addition to the set of functions that calculates performance measures and MCSE, the package also includes a function, create_skeleton(), that generates a skeleton outline for a simulation study. Another function, evaluate_by_row(), runs the simulation for each combination of conditions row by row. This function uses future_pmap() from the furrr package, making it easy to run the simulation in parallel (Vaughan \u0026amp; Dancho, 2018). The package also includes several datasets that contain results from example simulation studies.\nInstallation Install the latest release from CRAN:\ninstall.packages(\u0026quot;simhelpers\u0026quot;)  Install the development version from GitHub:\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;meghapsimatrix/simhelpers\u0026quot;)  ","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"bfaa4d5b29ba6edc1b222248de00df3f","permalink":"/project/internal-project/simhelpers/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/project/internal-project/simhelpers/","section":"project","summary":"Here is the website for the package.\nMonte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions (Morris, White, \u0026amp; Crowther, 2019). Methodologists use simulations to examine questions such as: (1) how does ordinary least squares regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? (3) how does cluster robust variance estimation perform when the number of clusters is small?","tags":["package","R","simhelpers","Monte Carlo","simulation"],"title":"simhelpers","type":"project"},{"authors":null,"categories":["R"],"content":"    I wanted to analyze the data from the April 2015 Nepal earthquake that resulted in around 10,000 deaths. I am using a dataset that I found in data.world. The data contains date, time, location and magnitude of the earthquake and the many aftershocks that followed. The data is updated as of June 2, 2015.\nNepal is my birthplace, my homeland. The earthquake was an extremely traumatic event for people who live there. Many people lost family members, their houses. I visited Nepal in 2017 and saw that every other house in Patan, Nepal (close to Kathmandu) was damaged. My relatives would talk about their experience of the earthquakes every day.\nLibraries library(tidyverse) library(geojsonio) library(broom) library(gganimate) library(leaflet) library(widgetframe)  Read in the data and clean earthquake_dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/meghapsimatrix/Data_Visualization/master/data/earthquake-0-csv-1.csv\u0026quot;) %\u0026gt;% mutate(lab = paste0(Epicentre, \u0026quot;; \u0026quot;, Date,\u0026quot;; Magnitude(ML): \u0026quot;, `Magnitude(ML)`)) head(earthquake_dat) ## # A tibble: 6 x 7 ## Date `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab ## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2015-06-01 04:35 28.0 85.5 4 Sindhupal‚Ä¶ Sindhup‚Ä¶ ## 2 2015-05-31 13:54 28.3 84.5 4.2 Lamjung Lamjung‚Ä¶ ## 3 2015-05-30 22:13 27.8 85.2 4.5 Nuwakot Nuwakot‚Ä¶ ## 4 2015-05-30 20:35 28.0 85.2 4 Rasuwa/Nu‚Ä¶ Rasuwa/‚Ä¶ ## 5 2015-05-30 01:52 27.8 85.2 4 Dhading /‚Ä¶ Dhading‚Ä¶ ## 6 2015-05-29 15:44 28 85.0 5.2 Dhading Dhading‚Ä¶ # there is one entry where I think the lat and long are switched summary(earthquake_dat$Latitude) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 26.94 27.71 27.82 28.06 27.98 84.71 summary(earthquake_dat$Longitude) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 28.16 85.23 85.80 85.25 86.06 86.67 # Gorkha seems like the lat and long are switched (outlier \u0026lt;- earthquake_dat %\u0026gt;% filter(Latitude == max(Latitude))) ## # A tibble: 1 x 7 ## Date `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab ## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2015-04-25 18:29 84.7 28.2 5.5 Gorkha Gorkha; ‚Ä¶ earthquake_dat \u0026lt;- earthquake_dat %\u0026gt;% mutate(Latitude = if_else(lab == outlier$lab \u0026amp; Date == outlier$Date, outlier$Longitude, Latitude), Longitude = if_else(lab == outlier$lab \u0026amp; Date == outlier$Date, outlier$Latitude, Longitude)) # Sindhupalchowk seems like the Longitude is wrong earthquake_dat %\u0026gt;% filter(Longitude == min(Longitude)) ## # A tibble: 1 x 7 ## Date `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab ## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2015-05-08 08:19 27.8 28.9 4.2 Sindhupal‚Ä¶ Sindhup‚Ä¶ sindhupalchowk \u0026lt;- earthquake_dat %\u0026gt;% filter(str_detect(Epicentre, \u0026quot;Sindhu\u0026quot;)) # Mean imputing based on other values for Sindhupalchowk earthquake_dat \u0026lt;- earthquake_dat %\u0026gt;% mutate(Longitude = if_else(Longitude == min(Longitude), mean(sindhupalchowk$Longitude), Longitude))  Make a Map of Nepal I got the code for the base map from here.\nnp \u0026lt;- geojson_read(\u0026quot;https://raw.githubusercontent.com/mesaugat/geoJSON-Nepal/master/nepal-districts.geojson\u0026quot;, what = \u0026quot;sp\u0026quot;) np_dat \u0026lt;- tidy(np) # plot np_plot \u0026lt;- ggplot() + geom_polygon(data = np_dat, aes( x = long, y = lat, group = group)) np_plot  Mapping on the Earthquake and Aftershocks Now plotting the latitude and longitudes. Size indicates the magnitude of the earthquake.\n(np_earthquake \u0026lt;- np_plot + geom_point(data = earthquake_dat, aes(x = Longitude, y = Latitude, size = `Magnitude(ML)`), color = \u0026quot;red\u0026quot;, alpha = .5) + labs(color = \u0026quot;\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;none\u0026quot;))  Animating np_animate \u0026lt;- np_earthquake + transition_states(Date) + labs(title = \u0026#39;Date: {closest_state}\u0026#39;) + enter_appear() + exit_disappear() animate(np_animate)  Leaflet Created using the leaflet package. Click on the dots on the map to learn the location, date, and the magnitude of the earthquake or aftershock.\nnp_leaf \u0026lt;- leaflet(earthquake_dat) %\u0026gt;% setView(lat = 27, lng = 85, zoom = 7) %\u0026gt;% addProviderTiles(providers$CartoDB.DarkMatter) %\u0026gt;% addCircleMarkers(~Longitude, ~Latitude, radius = ~`Magnitude(ML)`, fillOpacity = 0.5, popup = ~lab, stroke = FALSE) frameWidget(np_leaf)  {\"x\":{\"url\":\"/post/Nepal_Earthquake_files/figure-html//widgets/widget_unnamed-chunk-6.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]}  ","date":1574208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574208000,"objectID":"e3696d375d0ac0251c90bf6369ed4864","permalink":"/post/nepal_earthquake/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/post/nepal_earthquake/","section":"post","summary":"I wanted to analyze the data from the April 2015 Nepal earthquake that resulted in around 10,000 deaths. I am using a dataset that I found in data.world. The data contains date, time, location and magnitude of the earthquake and the many aftershocks that followed. The data is updated as of June 2, 2015.\nNepal is my birthplace, my homeland. The earthquake was an extremely traumatic event for people who live there.","tags":["Nepal","Nepal Earthquake 2015","gganimate","ggplot","leaflet"],"title":"Nepal Earthquake","type":"post"},{"authors":null,"categories":["R"],"content":"  Load the Data and Check Duplicates library(tidyverse) library(lubridate) library(kableExtra) library(ggridges) # there were complete duplicated rows dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv\u0026quot;) %\u0026gt;% distinct(.) # removes complete dups # check duplicates dup_title \u0026lt;- dat %\u0026gt;% filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %\u0026gt;% arrange(title) # examined they seem different movies even though same title dup_title %\u0026gt;% filter(duplicated(plot)) ## # A tibble: 0 x 12 ## # ‚Ä¶ with 12 variables: title \u0026lt;chr\u0026gt;, genres \u0026lt;chr\u0026gt;, release_date \u0026lt;chr\u0026gt;, ## # release_country \u0026lt;chr\u0026gt;, movie_rating \u0026lt;chr\u0026gt;, review_rating \u0026lt;dbl\u0026gt;, ## # movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;, language \u0026lt;chr\u0026gt;, ## # filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt; dup_title %\u0026gt;% filter(duplicated(release_date)| duplicated(release_date, fromLast = TRUE)) ## # A tibble: 2 x 12 ## title genres release_date release_country movie_rating review_rating ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 The ‚Ä¶ Comed‚Ä¶ 21-Jul-15 USA \u0026lt;NA\u0026gt; 5.2 ## 2 The ‚Ä¶ Comed‚Ä¶ 21-Jul-15 USA NOT RATED 3.6 ## # ‚Ä¶ with 6 more variables: movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;, ## # language \u0026lt;chr\u0026gt;, filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt; # The Jokesters seems to be a duplicate but with different rating and run time # Deleting it for now dat \u0026lt;- dat %\u0026gt;% filter(title != \u0026quot;The Jokesters (2015)\u0026quot;)  Genres The genre column looked extremely messy so some data munging fun. Each film can be categorized into multiple genres.\ndat_long \u0026lt;- dat %\u0026gt;% separate_rows(genres, sep = \u0026quot;\\\\|\u0026quot;) %\u0026gt;% # long format mutate(genres = str_trim(genres)) # Just to check - looks okay - just 1 movie with no genre table(dat_long$genres, useNA = \u0026quot;ifany\u0026quot;) ## ## Action Adult Adventure Animation Biography Comedy ## 335 1 115 39 4 511 ## Crime Drama Family Fantasy History Horror ## 120 529 11 229 6 3309 ## Music Musical Mystery Reality-TV Romance Sci-Fi ## 5 13 453 1 99 308 ## Sport Thriller War Western \u0026lt;NA\u0026gt; ## 4 1369 14 15 1 dat_long \u0026lt;- dat_long %\u0026gt;% mutate(genres = fct_infreq(fct_lump(genres, n = 8))) # Factor keeping 8 most frequent categories and lumping the rest to Other and order the factor by frequency Table: Number of Films per Genre genre_count \u0026lt;- dat_long %\u0026gt;% filter(!is.na(genres)) %\u0026gt;% group_by(genres) %\u0026gt;% summarize(n = n()) %\u0026gt;% ungroup() kable(genre_count, format = \u0026quot;html\u0026quot;, table.attr = \u0026quot;style = \\\u0026quot;color: white;\\\u0026quot;\u0026quot;) %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F)   genres  n      Horror  3309    Thriller  1369    Drama  529    Comedy  511    Mystery  453    Other  447    Action  335    Sci-Fi  308    Fantasy  229      Bar Graph: Distribution of Genres genre_count %\u0026gt;% ggplot(aes(x = genres, y = n, fill = genres)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + scale_y_continuous(labels = scales::comma) + # y axis to have commas scale_fill_brewer(palette =\u0026quot;BuPu\u0026quot;, direction = -1) + # reverse order the palette theme_light() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Films\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;)   Review Rating by Release Year Some of the years are dmy format, some just have the years. I am extracting the year and filling in any that didn‚Äôt parse with the year value from the original release_date column. No missing values for year :)\ndate_dat \u0026lt;- dat %\u0026gt;% mutate(date = dmy(release_date), yr = year(date), yr = ifelse(is.na(yr), release_date, yr)) table(is.na(date_dat$yr)) ## ## FALSE ## 3310 table(is.na(date_dat$review_rating)) ## ## FALSE TRUE ## 3058 252 date_dat %\u0026gt;% select(release_date, date, yr) %\u0026gt;% filter(is.na(date)) %\u0026gt;% head() ## # A tibble: 6 x 3 ## release_date date yr ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; ## 1 2017 NA 2017 ## 2 2013 NA 2013 ## 3 2012 NA 2012 ## 4 2013 NA 2013 ## 5 2017 NA 2017 ## 6 2017 NA 2017 date_dat %\u0026gt;% ggplot(aes(x = yr, y = review_rating, fill = yr)) + geom_boxplot(alpha = .5) + labs(x = \u0026quot;Release Year\u0026quot;, y = \u0026quot;Review Rating\u0026quot;) + theme_light() + theme(legend.position = \u0026quot;none\u0026quot;) Looks like there is a slight increase in ratings for newer films.\nAnd here is a ridgeline plot :)\ndate_dat %\u0026gt;% ggplot(aes(y = yr, x = review_rating, fill = yr)) + geom_density_ridges(alpha = .5) + labs(y = \u0026quot;Release Year\u0026quot;, x = \u0026quot;Review Rating\u0026quot;) + theme_light() + theme(legend.position = \u0026quot;none\u0026quot;)  ","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571616000,"objectID":"02aac3f369bab187be802f517dd2fed9","permalink":"/post/tidy_tues_horror/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/post/tidy_tues_horror/","section":"post","summary":"Load the Data and Check Duplicates library(tidyverse) library(lubridate) library(kableExtra) library(ggridges) # there were complete duplicated rows dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv\u0026quot;) %\u0026gt;% distinct(.) # removes complete dups # check duplicates dup_title \u0026lt;- dat %\u0026gt;% filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %\u0026gt;% arrange(title) # examined they seem different movies even though same title dup_title %\u0026gt;% filter(duplicated(plot)) ## # A tibble: 0 x 12 ## # ‚Ä¶ with 12 variables: title \u0026lt;chr\u0026gt;, genres \u0026lt;chr\u0026gt;, release_date \u0026lt;chr\u0026gt;, ## # release_country \u0026lt;chr\u0026gt;, movie_rating \u0026lt;chr\u0026gt;, review_rating \u0026lt;dbl\u0026gt;, ## # movie_run_time \u0026lt;chr\u0026gt;, plot \u0026lt;chr\u0026gt;, cast \u0026lt;chr\u0026gt;, language \u0026lt;chr\u0026gt;, ## # filming_locations \u0026lt;chr\u0026gt;, budget \u0026lt;chr\u0026gt; dup_title %\u0026gt;% filter(duplicated(release_date)| duplicated(release_date, fromLast = TRUE)) ## # A tibble: 2 x 12 ## title genres release_date release_country movie_rating review_rating ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 The ‚Ä¶ Comed‚Ä¶ 21-Jul-15 USA \u0026lt;NA\u0026gt; 5.","tags":["tidy tuesday","ggplot","tidyverse"],"title":"Tidy Tuesday Horror","type":"post"},{"authors":null,"categories":["R"],"content":" In my qualifying exam, in the written part, I was asked about how to analyze the effect of continuous, not binary, treatment using propensity score analysis. I skipped it for the written but I spent a few days looking up how to analyze this in case I would be asked during my oral examination. Sadly, no one asked me even when I asked them to, so here is a blog detailing my explorations.\nBinary Treatment For a review of propensity score analysis with binary treatment, please see Stuart (2010). Below let \\(e(X)\\) denote propensity scores, \\(D\\) denote a binary treatment, and \\(X\\) denote all the observed confounders. In the case of binary treatment, propensity scores represent the probability of receiving treatment given the covariates:\n\\[e(X) = P(D = 1|X)\\]\nWe estimate the scores using logistic regression or machine learning techniques like generalized boosted models.\n Extension to Contiuous Treatment In binary treatment context, we assume that the potential outcomes (\\(Y(1)\\) and \\(Y(0)\\)) are independent of treatment given \\(X\\):\n\\[Y(1), Y(0) \\perp\\!\\!\\!\\perp D |X\\]\nand by extension are independent given the propensity scores:\n\\[Y(0), Y(1) \\perp\\!\\!\\!\\perp D|e(X)\\]\nHirano and Imbens (2004) introduced the assumption of weak unconfoundedness in the context of continuous treatment. They stated: ‚Äúwe do not require joint independence of all potential outcomes. Instead, we require conditional independence to hold for each value of the treatment.‚Äù Below, let \\(T\\) denote a continuous treatment variable. The potential outcome when \\(T = t\\) is unreated to the treatment given the set of covariates:\n\\[Y(t) \\perp\\!\\!\\!\\perp T |X\\]\nTo calculate the propensity scores, in the case of continuous treatment, we cannot find the probability that continuous treatment (\\(T\\)) equals a given value \\(t\\). The likelihood of continuous variables taking on a given value is zero. For continuous treatment variable, we find the conditional density, the probability that \\(T\\) is infinitely close to \\(t\\) given \\(X\\). Below let \\(r(t,x)\\) denote the propensity scores. The right hand side of the equation represents the probability density function of a normal distribution. To estimate the propensity scores, we need to run a linear regression predicting the treatment by a set of covariates (Austin, 2019). From that we get the fitted values (\\(X\\hat{\\beta}\\)) and the model variance (\\({\\sigma}^2\\)) (Austin, 2019). The fitted values take the place of the mean in the density function.\n\\[ r(t, x) = {f_{T|X}}^{(t|x)} = \\frac{1}{\\sqrt{2\\pi\\hat{\\sigma}^2}} e^{-\\frac{(t - X\\hat{\\beta})^2}{2\\pi\\hat{\\sigma}^2}}\\]\nConditional on the propensity scores, we can assume that each potential outcome is independent of treatment:\n\\[Y(t) \\perp\\!\\!\\!\\perp T |r(t,x)\\]\nHirano and Imbens (2004) state that: ‚ÄúWithin strata with the same value of \\(r(t,X)\\), the probability that \\(T = t\\) does not depend on the value of \\(X\\).‚Äù I have seen \\(1\\) and \\(I\\) in front of the \\((T = t)\\), denoting the indicator function (Hirano \u0026amp; Imbens, 2004; Bia \u0026amp; Mattei, 2008).\n\\[X \\perp\\!\\!\\!\\perp 1(T = t)|r(t,x)\\]\n Calculating Weights Following the same logic as the inverse propensity weights (IPW) for the estimation of the average treatment effect (ATE) for a binary treatment, we calculate the inverse of the propensity scores as the weights:\n\\[\\frac{1}{{f_{T|X}}^{(t|x)}}\\]\nHowever, Robins et al. (2000) noted that such weights can result in infinite variance (Austin, 2019). They suggested to use stabilized weights as follows:\n\\[\\frac{{f_{T}}^{(t)}}{{f_{T|X}}^{(t|x)}}\\]\nHere the numerator represents the marginal density of treatment:\n\\[{f_{T}}^{(t)} = \\frac{1}{\\sqrt{2\\pi\\hat{\\sigma_t}^2}} e^{-\\frac{(t - \\mu_t)^2}{2\\pi\\hat{\\sigma_t}^2}}\\]\nThe stabilized weights make the distribution of the IPW narrower as there is less difference between the numerators and the denominators (van der Wal \u0026amp; Geskus, 2011).\n Real Data Analysis Example The data that I use here is from High School and Beyond (HSB) longitudinal study used by Rosenbaum (1986) to analyze the effect of dropping out of high school on later math achievement. The missing data in the original dataset have been replaced with one iteration of imputation using mice (van Buuren \u0026amp; Groothuis-Oudshoorn, 2011). This is not an appropriate method to analyze missing data but for the purpose of the example I am just using the one complete data. For the sake of this example, let‚Äôs analyze the effect of math efficacy on later math achievement.\nLoading the Data library(tidyverse) dat \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/meghapsimatrix/datasets/master/causal/HSLS09_incomplete.csv\u0026quot;)  The Numerators Here I am getting the numerators of the IPW, the marginal densities. I have regressed math_efficacy on just the intercept and used dnorm function to extract the densities.\n# the numerator mod_num \u0026lt;- lm(math_efficacy ~ 1, data = dat) num \u0026lt;- dnorm(x = dat$math_efficacy, # treatment mean = fitted.values(mod_num), # fitted values sd = summary(mod_num)$sigma) # model sigma  The Denominators Here I am getting the denominators of the IPW, the conditional densities. I have regressed math_efficacy on \\(X\\) and used dnorm function to extract the densities. I am not quite sure whether to use the model sigma which divides the sum of errors squared by the degrees of freedom before taking the square root or whether I should just take the standard deviation of the errors. However, with large sample size the difference between the two are negligible.\n# the demonimator mod_den \u0026lt;- lm(math_efficacy ~ sex + race + language + repeated_grade + IEP + locale + region + SES, data = dat) den \u0026lt;- dnorm(x = dat$math_efficacy, # treatment variable mean = fitted.values(mod_den), # fitted values sd = summary(mod_den)$sigma)  The IPW Below I calculate the stabilized weights:\ndat \u0026lt;- dat %\u0026gt;% mutate(ipw_s = num/den) summary(dat$ipw_s) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## 0.268 0.938 0.972 1.091 1.092 22.092 4223  Checking Balance and Outcome Analysis Please check back on my next post :) Short story: For balance, we have to calculate weighted correlations, and for outcome analysis we estimate the expected outcome for each treatment level and compare (Austin, 2019).\n  References Bia, M., \u0026amp; Mattei, A. (2008). A Stata package for the estimation of the dose-response function through adjustment for the generalized propensity score. The Stata Journal, 8(3), 354-373.\nHirano K and Imbens GW. The propensity score with continuous treatments. In: Gelman A and Meng X-L (eds) Applied Bayesian modeling and causal inference from incomplete-data perspectives. Chichester: John Wiley \u0026amp; Sons Ltd, 2004, pp.73‚Äì84.\nRobins JM, Hernan MA and Brumback B. Marginal structural models and causal inference in epidemiology. Epidemiol 2000; 11: 550‚Äì560.\nRosenbaum, P. R. (1986). Dropping out of high school in the United States: An observational study. Journal of Educational Statistics, 11(3), 207-224.\nStuart, E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics, 25(1), 1.\nvan Buuren, S., \u0026amp; Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in r. Journal of Statistical Software, 45 (3), 1‚Äì67. Retrieved from http://www.jstatsoft.org/v45/i03/\nvan der Wal, W. M., \u0026amp; Geskus, R. B. (2011). Ipw: an R package for inverse probability weighting. J Stat Softw, 43(13), 1-23.\nZhu, Y., Coffman, D. L., \u0026amp; Ghosh, D. (2015). A boosting algorithm for estimating generalized propensity scores with continuous treatments. Journal of causal inference, 3(1), 25-40.\n ","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"5923748eee57f4ce57894287497ab840","permalink":"/post/continuous-r-rmarkdown/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/post/continuous-r-rmarkdown/","section":"post","summary":"In my qualifying exam, in the written part, I was asked about how to analyze the effect of continuous, not binary, treatment using propensity score analysis. I skipped it for the written but I spent a few days looking up how to analyze this in case I would be asked during my oral examination. Sadly, no one asked me even when I asked them to, so here is a blog detailing my explorations.","tags":["propensity score analysis","causal inference","continuous treatment"],"title":"Continuous Treatment in Propensity Score Analysis","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Megha Joshi, Melissa L Aikens, Erin L Dolan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b96dcfe9f79c6bde909ed6fa02b8d38d","permalink":"/publication/mentoring/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/mentoring/","section":"publication","summary":"Direct ties to faculty related to better outcomes for undergraduate life-science researchers.","tags":["structural equation modeling","mentoring structures","undergraduate research"],"title":"Direct Ties to a Faculty Mentor Related to Positive Outcomes for Undergraduate Researchers","type":"publication"},{"authors":["Keenan A Pituch, Megha Joshi, Molly E Cain, Tiffany A Whittaker, Wanchen Chang, Ryoungsun Park, Graham J McDougall"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc4382c56f4947d13534da98a7570ca7","permalink":"/publication/missingdata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/missingdata/","section":"publication","summary":"Missing data treatment using multivariate methods for two-group comparisons and small sample sizes.","tags":["missing data","multivariate analysis","multilevel modeling","maximum likelihood"],"title":"The Performance of Multivariate Methods for Two-Group Comparisons with Small Samples and Incomplete Data","type":"publication"}]