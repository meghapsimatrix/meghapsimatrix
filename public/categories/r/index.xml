<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Megha</title>
    <link>/categories/r/</link>
      <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 21 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>R</title>
      <link>/categories/r/</link>
    </image>
    
    <item>
      <title>Unemployment Claims COVID-19</title>
      <link>/post/unemployment_claims/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/unemployment_claims/</guid>
      <description>


&lt;p&gt;In this post I am visualizing and analyzing the unprecedented increase in the number of unemployment claims filed in the US after the lockdown due to COVID 19 pandemic. I am retrieving the data from the &lt;code&gt;tidyquant&lt;/code&gt; package (Dancho &amp;amp; Vaughan, 2020).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(CausalImpact)
library(tidyverse)
library(scales)
library(tidyquant)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;icsa-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ICSA Data&lt;/h2&gt;
&lt;p&gt;Initial unemployment claims from the first date available, 1967:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icsa_dat &amp;lt;- &amp;quot;ICSA&amp;quot; %&amp;gt;%
  tq_get(get = &amp;quot;economic.data&amp;quot;,  
         from = &amp;quot;1967-01-07&amp;quot;) %&amp;gt;%
  rename(claims = price)


glimpse(icsa_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 2,789
## Columns: 3
## $ symbol &amp;lt;chr&amp;gt; &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;, &amp;quot;ICSA&amp;quot;‚Ä¶
## $ date   &amp;lt;date&amp;gt; 1967-01-07, 1967-01-14, 1967-01-21, 1967-01-28, 1967-02-04, 1‚Ä¶
## $ claims &amp;lt;int&amp;gt; 208000, 207000, 217000, 204000, 216000, 229000, 229000, 242000‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icsa_dat %&amp;gt;%
  ggplot(aes(x = date, y = claims)) + 
  geom_line(color = &amp;quot;blue&amp;quot;) + 
  scale_y_continuous(labels = comma) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Claims&amp;quot;, subtitle = &amp;quot;As of June 21, 2020&amp;quot;) + 
  ggtitle(&amp;quot;Unemployment Claims: 1967 to 2020&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/unemployment_claims_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-to-2008-recession&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison to 2008 Recession&lt;/h2&gt;
&lt;p&gt;In the graph below, I only selected 2008 to 2020. We can compare the unemployment claims during the 2008 recession to the number of claims filed during the COVID-19 lockdown. What is happening now is preposterous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icsa_dat %&amp;gt;%
  mutate(year = year(date)) %&amp;gt;%
  filter(year &amp;gt; 2007) %&amp;gt;%
  ggplot(aes(x = date, y = claims)) + 
  geom_line(color = &amp;quot;blue&amp;quot;) + 
  scale_y_continuous(labels = comma) +
  labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Claims&amp;quot;, subtitle = &amp;quot;As of June 21, 2020&amp;quot;) + 
  ggtitle(&amp;quot;Unemployment Claims: 2008 to 2020&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/unemployment_claims_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causal-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Causal Inference&lt;/h2&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Sometimes &lt;a href=&#34;https://twitter.com/hashtag/causalinference?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#causalinference&lt;/a&gt; is simple.&lt;br&gt;&lt;br&gt;‚ÄúWhat&#39;s the immediate causal effect of the &lt;a href=&#34;https://twitter.com/hashtag/COVID19?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#COVID19&lt;/a&gt; lockdowns on unemployment?‚Äù&lt;br&gt;&lt;br&gt;The answer is ‚ÄúUnprecedented‚Äù. &lt;br&gt;&lt;br&gt;We know we&#39;re in deep trouble when a time series is all we need. &lt;a href=&#34;https://t.co/cXK0wLw3no&#34;&gt;https://t.co/cXK0wLw3no&lt;/a&gt; &lt;a href=&#34;https://t.co/kS4PvVwihM&#34;&gt;pic.twitter.com/kS4PvVwihM&lt;/a&gt;
&lt;/p&gt;
‚Äî Miguel Hern√°n (&lt;span class=&#34;citation&#34;&gt;@_MiguelHernan&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/_MiguelHernan/status/1244215937978576898?ref_src=twsrc%5Etfw&#34;&gt;March 29, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Below, I use the &lt;code&gt;CausalImpact&lt;/code&gt; package to run a Bayesian structural time-series analysis (Brodersen et al., 2015). For more information on the package, please see this &lt;a href=&#34;https://cran.r-project.org/web/packages/CausalImpact/vignettes/CausalImpact.html&#34;&gt;vignette&lt;/a&gt;. Typically, it would be good to add covariates in the analysis but the data does not have any and given the rate of increase, I highly doubt that the inclusion of covariates would matter much. It would be interesting to compare the number of claims filed in US versus the number of claims filed in country with better social and economic security systems in place (perhaps the Netherlands). The impact of COVID-19 lockdowns on the number of unemployment claims is probably exacerbated by the lack of social and economic security in the US. In addition, due to employer based healthcare system in the US, millions of people have lost or are going to lose health insurance. Now more than every we need Medicare for All, $2000 a month stimulus, Green New Deal. The impact of climate change will be worse.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Projected‚¨ÜÔ∏èin unemployment:&lt;br&gt;&lt;br&gt;üá©üá™: 3.2%‚û°Ô∏è5.9%&lt;br&gt;üá¨üáß: 3.9%‚û°Ô∏è7%&lt;br&gt;üá´üá∑: 8.5%‚û°Ô∏è12%&lt;br&gt;üá∫üá∏: 3.5%‚û°Ô∏è32.1%&lt;br&gt;&lt;br&gt;Projected‚¨ÜÔ∏è in # of uninsured:&lt;br&gt;üá©üá™: 0&lt;br&gt;üá¨üáß: 0&lt;br&gt;üá´üá∑: 0&lt;br&gt;üá∫üá∏: At least 12 million&lt;br&gt;&lt;br&gt;Solution: Guarantee healthcare and paychecks like other wealthy countries do. &lt;a href=&#34;https://t.co/44ijS2evzL&#34;&gt;https://t.co/44ijS2evzL&lt;/a&gt;
&lt;/p&gt;
‚Äî Warren Gunnels (&lt;span class=&#34;citation&#34;&gt;@GunnelsWarren&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/GunnelsWarren/status/1252607696303513605?ref_src=twsrc%5Etfw&#34;&gt;April 21, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dates &amp;lt;- icsa_dat %&amp;gt;% 
  pull(date)

# create pre and post
pre_period &amp;lt;- c(dates[1], dates[2776])
post_period &amp;lt;- c(dates[2777], dates[length(dates)])

# make into dat
dat &amp;lt;- icsa_dat %&amp;gt;%
  select(date, y = claims)

# causal impact
impact &amp;lt;- CausalImpact(dat, pre_period, post_period)

sum_impact &amp;lt;- impact$summary %&amp;gt;%
  mutate(type = rownames(.)) %&amp;gt;%
  pivot_longer(cols = -type, 
               names_to = &amp;quot;stats&amp;quot;,
               values_to = &amp;quot;vals&amp;quot;) 

avg_impact &amp;lt;- sum_impact %&amp;gt;%
  mutate(vals = round(vals/1000000, 2))

rel_impact &amp;lt;- sum_impact %&amp;gt;%
  filter(str_detect(stats, &amp;quot;Rel&amp;quot;)) %&amp;gt;%
  mutate(vals = round(vals * 100))

# summary(impact, &amp;quot;report&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-report-causalimpact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis report: CausalImpact&lt;/h2&gt;
&lt;p&gt;Below is the report generated by &lt;code&gt;CausalImpact&lt;/code&gt; with some edits by me.&lt;/p&gt;
&lt;p&gt;Summing up the individual data points during the post-lockdown period, the total number of unemployment claims filed equaled 45.74M. By contrast, had the intervention not taken place, we would have expected a sum of 3.28M. The 95% interval of this prediction is [2.7M, 3.84M].&lt;/p&gt;
&lt;p&gt;The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Brodersen et al., 2015, Annals of Applied Statistics. Inferring causal impact using Bayesian
structural time-series models. &lt;a href=&#34;http://research.google.com/pubs/pub41854.html&#34; class=&#34;uri&#34;&gt;http://research.google.com/pubs/pub41854.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dancho, M. and Vaughan, D. (2020). tidyquant: Tidy Quantitative Financial Analysis. R
package version 1.0.0. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyquant&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyquant&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wickham, H. and Seidel, D. (2019). scales: Scale Functions for Visualization. R package
version 1.1.0. &lt;a href=&#34;https://CRAN.R-project.org/package=scales&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=scales&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43),
1686, &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34; class=&#34;uri&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Missing Data in Propensity Score Analysis</title>
      <link>/post/missing_dat/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/missing_dat/</guid>
      <description>


&lt;p&gt;Theories behind propensity score analysis assume that the covariates are fully observed &lt;span class=&#34;citation&#34;&gt;(Rosenbaum &amp;amp; Rubin, 1983, 1984)&lt;/span&gt;. However, in practice, observational analyses require large administrative databases or surveys, which inevitably will have missingness in the covariates. The response patterns of people with missing covariates may be different than those of people with observed data &lt;span class=&#34;citation&#34;&gt;(Mohan, Pearl, &amp;amp; Tian, 2013)&lt;/span&gt;. Therefore, ways to handle missing covariate data need to be examined. The basic estimation of propensity scores using logistic regression will delete cases with missing data, which can be problematic as it can cause bias in the treatment effect estimates &lt;span class=&#34;citation&#34;&gt;(Baraldi &amp;amp; Enders, 2010)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;missing-data-methods-in-propensity-score-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing Data Methods in Propensity Score Analysis&lt;/h2&gt;
&lt;p&gt;Below I explain three major methods used in the applied propensity score analysis literature when &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is not fully observed. I also explain three other methods to handle missing data that are not commonly used in applied literature but have been proposed theoretically. I also describe the assumptions about missing data and strong ignorability underlying each of the methods. Let &lt;span class=&#34;math inline&#34;&gt;\(X_{obs}\)&lt;/span&gt; indicate the observed parts of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{mis}\)&lt;/span&gt; indicate the missing parts of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; indicates the fully observed treatment indicator and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; indicates a fully observed outcome variable.&lt;/p&gt;
&lt;div id=&#34;complete-case-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Complete Case Analysis&lt;/h3&gt;
&lt;p&gt;This approach deletes cases with missing data in any of the variables used in the analysis &lt;span class=&#34;citation&#34;&gt;(Baraldi &amp;amp; Enders, 2010; Hill, 2004)&lt;/span&gt;. The traditional propensity score estimation method of using logistic regression implements complete case analysis by default. Therefore, this method is commonly used in applied research. The data that remains after deleting cases with missing data are assumed to be a simple random sample of the full data &lt;span class=&#34;citation&#34;&gt;(Baraldi &amp;amp; Enders, 2010)&lt;/span&gt;. Missingness is not related to any study variables nor to the hypothetically complete values of itself (Equations  and ). According to &lt;span class=&#34;citation&#34;&gt;Hill (2004)&lt;/span&gt;, the assumption underlying complete case analysis is that the joint distributions of &lt;span class=&#34;math inline&#34;&gt;\(X_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{mis}\)&lt;/span&gt; are same across the two treatment conditions:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_{obs}, X_{mis} \perp\!\!\!\perp D
\end{equation}\]&lt;/span&gt;
Therefore, an unbiased causal effect estimate can be retrieved after deleting cases with missing data. Such an assumption is very stringent and unlikely to be met in the types of data required for propensity score analyses &lt;span class=&#34;citation&#34;&gt;(Baraldi &amp;amp; Enders, 2010; Hill, 2004)&lt;/span&gt;. As mentioned above, deleting cases can also result in loss of power. Additionally, whether &lt;span class=&#34;math inline&#34;&gt;\(X_{mis}\)&lt;/span&gt; is balanced between the treatment groups cannot be confirmed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple Imputation&lt;/h3&gt;
&lt;p&gt;Multiple imputation (MI) generates multiple sets of data with the missing values drawn from an imputation model &lt;span class=&#34;citation&#34;&gt;(Mitra &amp;amp; Reiter, 2016; Rubin, 1987)&lt;/span&gt;. MI will create &lt;span class=&#34;math inline&#34;&gt;\(m &amp;gt; 1\)&lt;/span&gt; imputed datasets that contain different imputed values &lt;span class=&#34;citation&#34;&gt;(Murray, 2018; van Buuren, 2018)&lt;/span&gt;. Analyses can be performed on each of the datasets and results from each dataset can be aggregated across to derive a final estimate, standard error, degrees of freedom, and test result. Thus, MI involves two stages: (1) imputation and creation of the &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; imputed datasets, and (2) analysis and pooling of estimates across the datasets &lt;span class=&#34;citation&#34;&gt;(Murray, 2018; van Buuren, 2018)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;There are two approaches for imputing multivariate missing data: (1) joint modeling, JM, and (2) fully conditional specification, FCS, also called multivariate imputation by chained equations, MICE &lt;span class=&#34;citation&#34;&gt;(Murray, 2018; van Buuren, 2018; van Buuren &amp;amp; Groothuis-Oudshoorn, 2011)&lt;/span&gt;. JM entails jointly modeling variables with missingness by drawing from a multivariate distribution &lt;span class=&#34;citation&#34;&gt;(Murray, 2018; van Buuren, 2018; van Buuren &amp;amp; Groothuis-Oudshoorn, 2011)&lt;/span&gt;. FCS entails univariate conditional imputation models of variables with missing data that iteratively condition on all other variables using Monte Carlo Markov chain methods &lt;span class=&#34;citation&#34;&gt;(van Buuren, 2018; van Buuren &amp;amp; Groothuis-Oudshoorn, 2011)&lt;/span&gt;. JM imputes all variables simultaneously whereas FCS imputes one variable at a time &lt;span class=&#34;citation&#34;&gt;(van Buuren, 2018)&lt;/span&gt;. Because JM requires specification of a joint distribution for all the variables, it may not be as flexible as FCS when dealing with a large number of covariates with missing data &lt;span class=&#34;citation&#34;&gt;(Akande, Li, &amp;amp; Reiter, 2017)&lt;/span&gt;. However, FCS is computationally more intensive than JM &lt;span class=&#34;citation&#34;&gt;(van Buuren, 2018)&lt;/span&gt;. FCS also has been shown to outperform JM for categorical variables and is more robust under mis-specification of imputation model &lt;span class=&#34;citation&#34;&gt;(van Buuren, 2018)&lt;/span&gt;. Therefore, &lt;span class=&#34;citation&#34;&gt;van Buuren (2018)&lt;/span&gt; recommended to use FCS over JM.&lt;/p&gt;
&lt;p&gt;If the missingness mechanism is MAR or MCAR and if assumptions underlying the imputation model are correct, MI will yield unbiased results, as it uses the information available in &lt;span class=&#34;math inline&#34;&gt;\(X_{obs}\)&lt;/span&gt; to impute missing values &lt;span class=&#34;citation&#34;&gt;(Murray, 2018)&lt;/span&gt;. In the causal inference context, &lt;span class=&#34;citation&#34;&gt;Hill (2004)&lt;/span&gt; argued that MI relies on the assumption of &lt;em&gt;latent ignorability&lt;/em&gt;, a concept introduced by &lt;span class=&#34;citation&#34;&gt;Frangakis &amp;amp; Rubin (1999)&lt;/span&gt;. The assumption requires that the treatment assignment mechanism is ignorable given complete covariate data including the values that are latent or missing. These missing values are derived from MI. Below, let &lt;span class=&#34;math inline&#34;&gt;\(e_{MI}(X)\)&lt;/span&gt; denote propensity scores derived after multiple imputation:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_{obs}, X_{mis} \perp\!\!\!\perp D| e_{MI}(X)
\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
Y(1), Y(0) \perp\!\!\!\perp D | e_{MI}(X)
\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;citation&#34;&gt;Hill (2004)&lt;/span&gt; proposed two different ways to combine propensity scores estimated in each of the &lt;em&gt;m&lt;/em&gt; datasets:&lt;/p&gt;
&lt;div id=&#34;multiple-imputation-across-mi-across&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Multiple Imputation Across (MI Across)&lt;/h5&gt;
&lt;p&gt;This approach involves creating &lt;em&gt;m&lt;/em&gt; imputed datasets and then estimating propensity scores within each of the datasets and then averaging each unit‚Äôs &lt;em&gt;m&lt;/em&gt; propensity scores across the &lt;em&gt;m&lt;/em&gt; datasets &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. Stratification, matching or IPW can be implemented using these averaged propensity scores &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. Outcome models that include covariates will need to use the weights or strata derived from the averaged propensity scores and the &lt;em&gt;m&lt;/em&gt; sets of covariate values. The weighted regression estimates will then need to be pooled.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-within-mi-within&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Multiple Imputation Within (MI Within)&lt;/h5&gt;
&lt;p&gt;This approach involves creating &lt;em&gt;m&lt;/em&gt; imputed datasets and then estimating propensity scores within each of the datasets &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. Instead of averaging the propensity scores across the datasets, this method entails conditioning on the propensity scores within the datasets and running the outcome analyses within each dataset &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. The separate regression estimates have to be pooled.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generalized-propensity-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generalized Propensity Scores&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Rosenbaum &amp;amp; Rubin (1984)&lt;/span&gt; proposed the use of generalized propensity scores (GPS) as a way to address missing covariate data. The GPS represents the probability of treatment given observed covariates and missingness indicators &lt;span class=&#34;citation&#34;&gt;(Rosenbaum &amp;amp; Rubin, 1984)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
e^*(X) = P(D = 1|X_{obs}, R)
\end{equation}\]&lt;/span&gt;
Conditioning on &lt;span class=&#34;math inline&#34;&gt;\(e^*(X)\)&lt;/span&gt; will balance the treatment groups in terms of the observed covariates and missingness patterns &lt;span class=&#34;citation&#34;&gt;(Rosenbaum &amp;amp; Rubin, 1984)&lt;/span&gt;. The observed part of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the missingness pattern indicators, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;, will be independent of treatment assignment given the GPS &lt;span class=&#34;citation&#34;&gt;(Rosenbaum &amp;amp; Rubin, 1984)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_{obs}, R \perp\!\!\!\perp D| e^*(X)
\end{equation}\]&lt;/span&gt;
However, conditioning on GPS will not balance the groups in terms of the unobserved values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(Rosenbaum &amp;amp; Rubin, 1984)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_{mis} \not\!\perp\!\!\!\perp D| e^*(X)
\end{equation}\]&lt;/span&gt;
Although this technique of treating missing data is not generally recommended for other types of missing data analyses, it has been recommended for use in propensity score analysis literature &lt;span class=&#34;citation&#34;&gt;(Rosenbaum &amp;amp; Rubin, 1984; Stuart, 2010)&lt;/span&gt;. In the context of propensity score analysis, this approach does not assume latent ignorability of treatment assignment because legitimate values for missing data are never derived. The assumption underlying this method is that balancing the treatment and control groups on &lt;span class=&#34;math inline&#34;&gt;\(X_{obs}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is a sufficient condition to satisfy ignorability. With the GPS, the treatment and control groups are possibly not going to be balanced in terms of &lt;span class=&#34;math inline&#34;&gt;\(X_{mis}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For large studies with few missing data patterns, &lt;span class=&#34;citation&#34;&gt;Rosenbaum &amp;amp; Rubin (1984)&lt;/span&gt; suggested estimating separate logit models for each missingness pattern. In practice, it is common to encounter many patterns of missing data. For these scenarios, &lt;span class=&#34;citation&#34;&gt;Rosenbaum &amp;amp; Rubin (1984)&lt;/span&gt; suggested creating an additional category indicating missingness for categorical variables. For continuous variables, &lt;span class=&#34;citation&#34;&gt;Stuart (2010)&lt;/span&gt; recommended imputing missing data with a single arbitrary value, such as the overall mean of the covariate, and then creating a missingness indicator variable. In general missing data analysis context, &lt;span class=&#34;citation&#34;&gt;van Buuren (2018)&lt;/span&gt; noted that this method of combining arbitrary (mean) imputation along with missingness indicators can underestimate the standard error of the estimate of interest.&lt;/p&gt;
&lt;p&gt;The CART algorithms treat missing data natively as they split missingness as a category itself. In this manner, this approach is similar to the GPS which uses missingness pattern indicators when estimating propensity scores. The missingness categories are used to estimate propensity scores and conditioning on the propensity scores should balance the treatment and control condition in terms of the patterns. However, splitting does not actually impute the missing data so it is plausible to assume that like GPS, scores derived using the splitting method will not balance the groups in terms of the latent missing data. In addition, unlike MI, there are no imputed complete datasets saved to analyze for the outcome model. Therefore, splitting would need to be combined with some other technique for outcome modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Methods&lt;/h3&gt;
&lt;p&gt;The following methods have been discussed theoretically in literature examining missing data methods in propensity score analysis. However, these methods are not commonly used in applied literature.&lt;/p&gt;
&lt;div id=&#34;complete-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complete Variables&lt;/h4&gt;
&lt;p&gt;This method removes any variable with missing data &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. By removing variables with missing data, the approach assumes that the distribution of those variables (both the observed and missing parts) are the same across the two treatment groups &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. If this assumption does not hold, then this method can result in bias in treatment effect estimates due to removal of important confounding variables &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dagostino-and-rubin-expectation-maximization&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;D‚ÄôAgostino and Rubin Expectation Maximization&lt;/h4&gt;
&lt;p&gt;Another approach is a method introduced by &lt;span class=&#34;citation&#34;&gt;D‚ÄôAgostino &amp;amp; Rubin (2000)&lt;/span&gt;, which estimates propensity scores using an Expectation Conditional Maximization (ECM) algorithm &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. This method, DR, works similar to GPS as it models &lt;span class=&#34;math inline&#34;&gt;\(X_{obs}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;, and the treatment indicator variable. However, instead of imputing &lt;span class=&#34;math inline&#34;&gt;\(X_{mis}\)&lt;/span&gt;, the DR method uses ECM to estimate propensity scores in presence of missing data &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;. The assumption underlying DR is that within each missingness pattern defined by &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_{mis}\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; given the observed data, &lt;span class=&#34;math inline&#34;&gt;\(X_{obs}\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(Hill, 2004)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_{mis} \perp\!\!\!\perp D| X_{obs}, R
\end{equation}\]&lt;/span&gt;
Such independence is sufficient to satisfy the ignorability assumption in presence of missing covariate data. With this method, the assumption cannot be checked, however, as DR does not actually impute the missing values. This method is not readily available in commonly used software like R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-imputation-missingness-indicator-pattern-mixutre&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiple Imputation Missingness Indicator Pattern Mixutre&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Qu &amp;amp; Lipkovich (2009)&lt;/span&gt; extended MI by introducing the missingness indicator pattern mixture (MIMP) approach, which is the same as MI but adds &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in the propensity score estimation model. The rationale behind this approach is to use information given by missingness patterns to estimate treatment propensities. The method will assume latent ignorabilty. However, this approach should also balance the treatment group on &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is used to estimate the propensity scores:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_{obs}, X_{mis}, R \perp\!\!\!\perp D| e_{MIMP}(X)
\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;citation&#34;&gt;Qu &amp;amp; Lipkovich (2009)&lt;/span&gt; argued that extending MI by adding R to the propensity score estimation accounts for non-ignorability or MNAR &lt;span class=&#34;citation&#34;&gt;(Qu &amp;amp; Lipkovich, 2009; van Buuren, 2018)&lt;/span&gt;. This method allows missingness itself to provide information on missingness:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
P(X| X_{obs}, R = 1) \neq P(X| X_{obs}, R = 0)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-akande2017empirical&#34;&gt;
&lt;p&gt;Akande, O., Li, F., &amp;amp; Reiter, J. (2017). An empirical comparison of multiple imputation methods for categorical data. &lt;em&gt;The American Statistician&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;(2), 162‚Äì170.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-baraldi_introduction_2010&#34;&gt;
&lt;p&gt;Baraldi, A. N., &amp;amp; Enders, C. K. (2010). An introduction to modern missing data analyses. &lt;em&gt;Journal of School Psychology&lt;/em&gt;, &lt;em&gt;48&lt;/em&gt;(1), 5‚Äì37. &lt;a href=&#34;https://doi.org/10.1016/j.jsp.2009.10.001&#34;&gt;https://doi.org/10.1016/j.jsp.2009.10.001&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-dagostino_estimating_2000&#34;&gt;
&lt;p&gt;D‚ÄôAgostino, R. B., &amp;amp; Rubin, D. B. (2000). Estimating and Using Propensity Scores with Partially Missing Data. &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;, &lt;em&gt;95&lt;/em&gt;(451), 749. &lt;a href=&#34;https://doi.org/10.2307/2669455&#34;&gt;https://doi.org/10.2307/2669455&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-frangakis_addressing_1999&#34;&gt;
&lt;p&gt;Frangakis, C., &amp;amp; Rubin, D. B. (1999). Addressing complications of intention-to-treat analysis in the combined presence of all-or-none treatment-noncompliance and subsequent missing outcomes. &lt;em&gt;Biometrika&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(2), 365‚Äì379. &lt;a href=&#34;https://doi.org/10.1093/biomet/86.2.365&#34;&gt;https://doi.org/10.1093/biomet/86.2.365&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hill_2004&#34;&gt;
&lt;p&gt;Hill, J. (2004). &lt;em&gt;Reducing bias in treatment effect estimation in observational studies suffering from missing data&lt;/em&gt;. Columbia University Institute for Social &amp;amp; Economic Research &amp;amp; Policy (ISERP).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mitra_comparison_2016&#34;&gt;
&lt;p&gt;Mitra, R., &amp;amp; Reiter, J. P. (2016). A comparison of two methods of estimating propensity scores after multiple imputation. &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 188‚Äì204. &lt;a href=&#34;https://doi.org/10.1177/0962280212445945&#34;&gt;https://doi.org/10.1177/0962280212445945&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mohan_graphical_2013&#34;&gt;
&lt;p&gt;Mohan, K., Pearl, J., &amp;amp; Tian, J. (2013). Graphical models for inference with missing data. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, &amp;amp; K. Q. Weinberger (Eds.), &lt;em&gt;Advances in neural information processing system&lt;/em&gt; (pp. 1277‚Äì1285). Red Hook, NY: Curran Associates, Inc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-murray_multiple_2018&#34;&gt;
&lt;p&gt;Murray, J. S. (2018). Multiple Imputation: A Review of Practical and Theoretical Findings. &lt;em&gt;Statistical Science&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(2), 142‚Äì159. &lt;a href=&#34;https://doi.org/10.1214/18-STS644&#34;&gt;https://doi.org/10.1214/18-STS644&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-qu_propensity_2009&#34;&gt;
&lt;p&gt;Qu, Y., &amp;amp; Lipkovich, I. (2009). Propensity score estimation with missing values using a multiple imputation missingness pattern (MIMP) approach. &lt;em&gt;Statistics in Medicine&lt;/em&gt;, &lt;em&gt;28&lt;/em&gt;(9), 1402‚Äì1414. &lt;a href=&#34;https://doi.org/10.1002/sim.3549&#34;&gt;https://doi.org/10.1002/sim.3549&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaum_central_1983&#34;&gt;
&lt;p&gt;Rosenbaum, P. R., &amp;amp; Rubin, D. B. (1983). The Central Role of the Propensity Score in Observational Studies for Causal Effects. &lt;em&gt;Biometrika&lt;/em&gt;, &lt;em&gt;70&lt;/em&gt;(1), 41. &lt;a href=&#34;https://doi.org/10.2307/2335942&#34;&gt;https://doi.org/10.2307/2335942&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaum_reducing_1984&#34;&gt;
&lt;p&gt;Rosenbaum, P. R., &amp;amp; Rubin, D. B. (1984). Reducing Bias in Observational Studies Using Subclassification on the Propensity Score. &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;, &lt;em&gt;79&lt;/em&gt;(387), 516. &lt;a href=&#34;https://doi.org/10.2307/2288398&#34;&gt;https://doi.org/10.2307/2288398&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rubin_multiple_1987&#34;&gt;
&lt;p&gt;Rubin, D. B. (1987). &lt;em&gt;Multiple imputation for nonresponse in surveys&lt;/em&gt;. New York: Wiley.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stuart_matching_2010&#34;&gt;
&lt;p&gt;Stuart, E. A. (2010). Matching Methods for Causal Inference: A Review and a Look Forward. &lt;em&gt;Statistical Science&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 1‚Äì21. &lt;a href=&#34;https://doi.org/10.1214/09-STS313&#34;&gt;https://doi.org/10.1214/09-STS313&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-van2018flexible&#34;&gt;
&lt;p&gt;van Buuren, S. (2018). &lt;em&gt;Flexible imputation of missing data&lt;/em&gt;. Chapman; Hall/CRC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanB_2011&#34;&gt;
&lt;p&gt;van Buuren, S., &amp;amp; Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in r. &lt;em&gt;Journal of Statistical Software&lt;/em&gt;, &lt;em&gt;45&lt;/em&gt;(3), 1‚Äì67. Retrieved from &lt;a href=&#34;http://www.jstatsoft.org/v45/i03/&#34;&gt;http://www.jstatsoft.org/v45/i03/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nepal Earthquake</title>
      <link>/post/nepal_earthquake/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/nepal_earthquake/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I wanted to analyze the data from the &lt;a href=&#34;https://en.wikipedia.org/wiki/April_2015_Nepal_earthquake&#34;&gt;April 2015 Nepal earthquake&lt;/a&gt; that resulted in around 10,000 deaths. I am using a &lt;a href=&#34;https://data.world/opennepal/1b7b5d6e-3c98-49f4-a884-a167c4040d3a&#34;&gt;dataset&lt;/a&gt; that I found in &lt;code&gt;data.world&lt;/code&gt;. The data contains date, time, location and magnitude of the earthquake and the many aftershocks that followed. The data is updated as of June 2, 2015.&lt;/p&gt;
&lt;p&gt;Nepal is my birthplace, my homeland. The earthquake was an extremely traumatic event for people who live there. Many people lost family members, their houses. I visited Nepal in 2017 and saw that every other house in Patan, Nepal (close to Kathmandu) was damaged. My relatives would talk about their experience of the earthquakes every day.&lt;/p&gt;
&lt;div id=&#34;libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(geojsonio)
library(broom)
library(gganimate)
library(leaflet)
library(widgetframe)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-in-the-data-and-clean&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Read in the data and clean&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earthquake_dat &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/meghapsimatrix/Data_Visualization/master/data/earthquake-0-csv-1.csv&amp;quot;) %&amp;gt;%
  mutate(lab = paste0(Epicentre, &amp;quot;; &amp;quot;, Date,&amp;quot;; Magnitude(ML): &amp;quot;, `Magnitude(ML)`))

head(earthquake_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   Date       `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre  lab     
##   &amp;lt;date&amp;gt;     &amp;lt;time&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;   
## 1 2015-06-01 04:35            28.0      85.5             4   Sindhupal‚Ä¶ Sindhup‚Ä¶
## 2 2015-05-31 13:54            28.3      84.5             4.2 Lamjung    Lamjung‚Ä¶
## 3 2015-05-30 22:13            27.8      85.2             4.5 Nuwakot    Nuwakot‚Ä¶
## 4 2015-05-30 20:35            28.0      85.2             4   Rasuwa/Nu‚Ä¶ Rasuwa/‚Ä¶
## 5 2015-05-30 01:52            27.8      85.2             4   Dhading /‚Ä¶ Dhading‚Ä¶
## 6 2015-05-29 15:44            28        85.0             5.2 Dhading    Dhading‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there is one entry where I think the lat and long are switched
summary(earthquake_dat$Latitude)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   26.94   27.71   27.82   28.06   27.98   84.71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(earthquake_dat$Longitude)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   28.16   85.23   85.80   85.25   86.06   86.67&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Gorkha seems like the lat and long are switched
(outlier &amp;lt;- earthquake_dat %&amp;gt;%
  filter(Latitude == max(Latitude)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 7
##   Date       `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre lab      
##   &amp;lt;date&amp;gt;     &amp;lt;time&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;    
## 1 2015-04-25 18:29            84.7      28.2             5.5 Gorkha    Gorkha; ‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;earthquake_dat &amp;lt;- earthquake_dat %&amp;gt;%
  mutate(Latitude = if_else(lab == outlier$lab &amp;amp; Date == outlier$Date, outlier$Longitude, Latitude),
         Longitude = if_else(lab == outlier$lab &amp;amp; Date == outlier$Date, outlier$Latitude, Longitude))

# Sindhupalchowk seems like the Longitude is wrong
earthquake_dat %&amp;gt;%
  filter(Longitude == min(Longitude))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 7
##   Date       `Local Time` Latitude Longitude `Magnitude(ML)` Epicentre  lab     
##   &amp;lt;date&amp;gt;     &amp;lt;time&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;   
## 1 2015-05-08 08:19            27.8      28.9             4.2 Sindhupal‚Ä¶ Sindhup‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sindhupalchowk &amp;lt;- earthquake_dat %&amp;gt;%
  filter(str_detect(Epicentre, &amp;quot;Sindhu&amp;quot;))

# Mean imputing based on other values for Sindhupalchowk
earthquake_dat &amp;lt;- earthquake_dat %&amp;gt;%
  mutate(Longitude = if_else(Longitude == min(Longitude), mean(sindhupalchowk$Longitude), Longitude))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-a-map-of-nepal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Make a Map of Nepal&lt;/h1&gt;
&lt;p&gt;I got the code for the base map from &lt;a href=&#34;https://stackoverflow.com/questions/50859765/chloropleth-map-with-geojson-and-ggplot2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;np &amp;lt;- geojson_read(&amp;quot;https://raw.githubusercontent.com/mesaugat/geoJSON-Nepal/master/nepal-districts.geojson&amp;quot;,  what = &amp;quot;sp&amp;quot;)
np_dat &amp;lt;- tidy(np)


# plot
np_plot &amp;lt;- ggplot() +
  geom_polygon(data = np_dat, aes( x = long, y = lat, group = group)) 

np_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nepal_Earthquake_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mapping-on-the-earthquake-and-aftershocks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mapping on the Earthquake and Aftershocks&lt;/h1&gt;
&lt;p&gt;Now plotting the latitude and longitudes. Size indicates the magnitude of the earthquake.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(np_earthquake &amp;lt;- np_plot + 
  geom_point(data = earthquake_dat, 
             aes(x = Longitude, y = Latitude, 
                 size = `Magnitude(ML)`), 
             color = &amp;quot;red&amp;quot;, alpha = .5) + 
  labs(color = &amp;quot;&amp;quot;) + 
  theme_void() +
  theme(legend.position = &amp;quot;none&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nepal_Earthquake_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animating&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Animating&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;np_animate &amp;lt;- np_earthquake +
  transition_states(Date) +
  labs(title = &amp;#39;Date: {closest_state}&amp;#39;) +
  enter_appear() +
  exit_disappear()

animate(np_animate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Nepal_Earthquake_files/figure-html/unnamed-chunk-5-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;leaflet&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Leaflet&lt;/h1&gt;
&lt;p&gt;Created using the &lt;code&gt;leaflet&lt;/code&gt; package. Click on the dots on the map to learn the location, date, and the magnitude of the earthquake or aftershock.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;np_leaf &amp;lt;- leaflet(earthquake_dat) %&amp;gt;% 
  setView(lat = 27, lng = 85, zoom = 7) %&amp;gt;%
  addProviderTiles(providers$CartoDB.DarkMatter) %&amp;gt;%
  addCircleMarkers(~Longitude, ~Latitude,
                   radius = ~`Magnitude(ML)`, fillOpacity = 0.5,
                   popup = ~lab, stroke = FALSE)

frameWidget(np_leaf)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/Nepal_Earthquake_files/figure-html//widgets/widget_unnamed-chunk-6.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Tuesday Horror</title>
      <link>/post/tidy_tues_horror/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/tidy_tues_horror/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;load-the-data-and-check-duplicates&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load the Data and Check Duplicates&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(kableExtra)
library(ggridges)


# there were complete duplicated rows
dat &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv&amp;quot;) %&amp;gt;%
  distinct(.) # removes complete dups

# check duplicates
dup_title &amp;lt;- dat %&amp;gt;%
  filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %&amp;gt;%
  arrange(title)

# examined they seem different movies even though same title
dup_title %&amp;gt;%
  filter(duplicated(plot))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 0 x 12
## # ‚Ä¶ with 12 variables: title &amp;lt;chr&amp;gt;, genres &amp;lt;chr&amp;gt;, release_date &amp;lt;chr&amp;gt;,
## #   release_country &amp;lt;chr&amp;gt;, movie_rating &amp;lt;chr&amp;gt;, review_rating &amp;lt;dbl&amp;gt;,
## #   movie_run_time &amp;lt;chr&amp;gt;, plot &amp;lt;chr&amp;gt;, cast &amp;lt;chr&amp;gt;, language &amp;lt;chr&amp;gt;,
## #   filming_locations &amp;lt;chr&amp;gt;, budget &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dup_title %&amp;gt;%
  filter(duplicated(release_date)| duplicated(release_date, fromLast = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 12
##   title genres release_date release_country movie_rating review_rating
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;
## 1 The ‚Ä¶ Comed‚Ä¶ 21-Jul-15    USA             &amp;lt;NA&amp;gt;                   5.2
## 2 The ‚Ä¶ Comed‚Ä¶ 21-Jul-15    USA             NOT RATED              3.6
## # ‚Ä¶ with 6 more variables: movie_run_time &amp;lt;chr&amp;gt;, plot &amp;lt;chr&amp;gt;, cast &amp;lt;chr&amp;gt;,
## #   language &amp;lt;chr&amp;gt;, filming_locations &amp;lt;chr&amp;gt;, budget &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The Jokesters seems to be a duplicate but with different rating and run time
# Deleting it for now
dat &amp;lt;- dat %&amp;gt;%
  filter(title != &amp;quot;The Jokesters (2015)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;genres&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Genres&lt;/h1&gt;
&lt;p&gt;The genre column looked extremely messy so some data munging fun. Each film can be categorized into multiple genres.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_long &amp;lt;- dat %&amp;gt;% 
  separate_rows(genres, sep = &amp;quot;\\|&amp;quot;) %&amp;gt;% # long format
  mutate(genres = str_trim(genres))  

# Just to check - looks okay - just 1 movie with no genre
table(dat_long$genres, useNA = &amp;quot;ifany&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##     Action      Adult  Adventure  Animation  Biography     Comedy 
##        335          1        115         39          4        511 
##      Crime      Drama     Family    Fantasy    History     Horror 
##        120        529         11        229          6       3309 
##      Music    Musical    Mystery Reality-TV    Romance     Sci-Fi 
##          5         13        453          1         99        308 
##      Sport   Thriller        War    Western       &amp;lt;NA&amp;gt; 
##          4       1369         14         15          1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_long &amp;lt;- dat_long %&amp;gt;%
  mutate(genres = fct_infreq(fct_lump(genres, n = 8))) # Factor keeping 8 most frequent categories and lumping the rest to Other and order the factor by frequency&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;table-number-of-films-per-genre&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Table: Number of Films per Genre&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genre_count &amp;lt;- dat_long %&amp;gt;%
  filter(!is.na(genres)) %&amp;gt;%
  group_by(genres) %&amp;gt;%
  summarize(n = n()) %&amp;gt;%
  ungroup() 

kable(genre_count, format = &amp;quot;html&amp;quot;, table.attr = &amp;quot;style = \&amp;quot;color: white;\&amp;quot;&amp;quot;) %&amp;gt;%
  kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;, full_width = F)&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;color: white; width: auto !important; margin-left: auto; margin-right: auto;&#34; class=&#34;table table-striped&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
genres
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Horror
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3309
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thriller
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1369
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Drama
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
529
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Comedy
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
511
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mystery
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
453
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
447
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Action
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
335
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sci-Fi
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
308
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fantasy
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
229
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;bar-graph-distribution-of-genres&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bar Graph: Distribution of Genres&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genre_count %&amp;gt;%
  ggplot(aes(x = genres, y = n, fill = genres)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  scale_y_continuous(labels = scales::comma) +  # y axis to have commas 
  scale_fill_brewer(palette =&amp;quot;BuPu&amp;quot;, direction = -1) + # reverse order the palette
  theme_light() + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Films&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tidy_tues_horror_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;review-rating-by-release-year&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Review Rating by Release Year&lt;/h1&gt;
&lt;p&gt;Some of the years are &lt;code&gt;dmy&lt;/code&gt; format, some just have the years. I am extracting the year and filling in any that didn‚Äôt parse with the year value from the original release_date column. No missing values for year :)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;date_dat &amp;lt;- dat %&amp;gt;%
  mutate(date = dmy(release_date),
         yr = year(date),
         yr = ifelse(is.na(yr), release_date, yr))

table(is.na(date_dat$yr))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE 
##  3310&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(is.na(date_dat$review_rating))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##  3058   252&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;date_dat %&amp;gt;%
  select(release_date, date, yr) %&amp;gt;%
  filter(is.na(date)) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   release_date date       yr   
##   &amp;lt;chr&amp;gt;        &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;
## 1 2017         NA         2017 
## 2 2013         NA         2013 
## 3 2012         NA         2012 
## 4 2013         NA         2013 
## 5 2017         NA         2017 
## 6 2017         NA         2017&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;date_dat %&amp;gt;%  
  ggplot(aes(x = yr, y = review_rating, fill = yr)) +
  geom_boxplot(alpha = .5) + 
  labs(x = &amp;quot;Release Year&amp;quot;, y = &amp;quot;Review Rating&amp;quot;) + 
  theme_light()  + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tidy_tues_horror_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like there is a slight increase in ratings for newer films.&lt;/p&gt;
&lt;p&gt;And here is a ridgeline plot :)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;date_dat %&amp;gt;%  
  ggplot(aes(y = yr, x = review_rating, fill = yr)) +
  geom_density_ridges(alpha = .5) + 
  labs(y = &amp;quot;Release Year&amp;quot;, x = &amp;quot;Review Rating&amp;quot;) +
  theme_light() + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tidy_tues_horror_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Continuous Treatment in Propensity Score Analysis</title>
      <link>/post/continuous-r-rmarkdown/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/continuous-r-rmarkdown/</guid>
      <description>


&lt;p&gt;In my qualifying exam, in the written part, I was asked about how to analyze the effect of continuous, not binary, treatment using propensity score analysis. I skipped it for the written but I spent a few days looking up how to analyze this in case I would be asked during my oral examination. Sadly, no one asked me even when I asked them to, so here is a blog detailing my explorations.&lt;/p&gt;
&lt;div id=&#34;binary-treatment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Binary Treatment&lt;/h1&gt;
&lt;p&gt;For a review of propensity score analysis with binary treatment, please see Stuart (2010). Below let &lt;span class=&#34;math inline&#34;&gt;\(e(X)\)&lt;/span&gt; denote propensity scores, &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; denote a binary treatment, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; denote all the observed confounders. In the case of binary treatment, propensity scores represent the probability of receiving treatment given the covariates:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[e(X) = P(D = 1|X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We estimate the scores using logistic regression or machine learning techniques like generalized boosted models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extension-to-contiuous-treatment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Extension to Contiuous Treatment&lt;/h1&gt;
&lt;p&gt;In binary treatment context, we assume that the potential outcomes (&lt;span class=&#34;math inline&#34;&gt;\(Y(1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y(0)\)&lt;/span&gt;) are independent of treatment given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y(1), Y(0) \perp\!\!\!\perp D |X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and by extension are independent given the propensity scores:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y(0), Y(1) \perp\!\!\!\perp D|e(X)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hirano and Imbens (2004) introduced the assumption of weak unconfoundedness in the context of continuous treatment. They stated: ‚Äúwe do not require joint independence of all potential outcomes. Instead, we require conditional independence to hold for each value of the treatment.‚Äù Below, let &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; denote a continuous treatment variable. The potential outcome when &lt;span class=&#34;math inline&#34;&gt;\(T = t\)&lt;/span&gt; is unreated to the treatment given the set of covariates:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y(t) \perp\!\!\!\perp T |X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To calculate the propensity scores, in the case of continuous treatment, we cannot find the probability that continuous treatment (&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;) equals a given value &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. The likelihood of continuous variables taking on a given value is zero. For continuous treatment variable, we find the conditional density, the probability that &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is infinitely close to &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Below let &lt;span class=&#34;math inline&#34;&gt;\(r(t,x)\)&lt;/span&gt; denote the propensity scores. The right hand side of the equation represents the probability density function of a normal distribution. To estimate the propensity scores, we need to run a linear regression predicting the treatment by a set of covariates (Austin, 2019). From that we get the fitted values (&lt;span class=&#34;math inline&#34;&gt;\(X\hat{\beta}\)&lt;/span&gt;) and the model variance (&lt;span class=&#34;math inline&#34;&gt;\({\sigma}^2\)&lt;/span&gt;) (Austin, 2019). The fitted values take the place of the mean in the density function.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ r(t, x) = {f_{T|X}}^{(t|x)} = \frac{1}{\sqrt{2\pi\hat{\sigma}^2}} e^{-\frac{(t - X\hat{\beta})^2}{2\pi\hat{\sigma}^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Conditional on the propensity scores, we can assume that each potential outcome is independent of treatment:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y(t) \perp\!\!\!\perp T |r(t,x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hirano and Imbens (2004) state that: ‚ÄúWithin strata with the same value of &lt;span class=&#34;math inline&#34;&gt;\(r(t,X)\)&lt;/span&gt;, the probability that &lt;span class=&#34;math inline&#34;&gt;\(T = t\)&lt;/span&gt; does not depend on the value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.‚Äù I have seen &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; in front of the &lt;span class=&#34;math inline&#34;&gt;\((T = t)\)&lt;/span&gt;, denoting the indicator function (Hirano &amp;amp; Imbens, 2004; Bia &amp;amp; Mattei, 2008).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X \perp\!\!\!\perp 1(T = t)|r(t,x)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-weights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Calculating Weights&lt;/h1&gt;
&lt;p&gt;Following the same logic as the inverse propensity weights (IPW) for the estimation of the average treatment effect (ATE) for a binary treatment, we calculate the inverse of the propensity scores as the weights:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{{f_{T|X}}^{(t|x)}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, Robins et al. (2000) noted that such weights can result in infinite variance (Austin, 2019). They suggested to use stabilized weights as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{{f_{T}}^{(t)}}{{f_{T|X}}^{(t|x)}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here the numerator represents the marginal density of treatment:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{f_{T}}^{(t)} = \frac{1}{\sqrt{2\pi\hat{\sigma_t}^2}} e^{-\frac{(t - \mu_t)^2}{2\pi\hat{\sigma_t}^2}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The stabilized weights make the distribution of the IPW narrower as there is less difference between the numerators and the denominators (van der Wal &amp;amp; Geskus, 2011).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-data-analysis-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Real Data Analysis Example&lt;/h1&gt;
&lt;p&gt;The data that I use here is from High School and Beyond (HSB) longitudinal study used by Rosenbaum (1986) to analyze the effect of dropping out of high school on later math achievement. The missing data in the original dataset have been replaced with one iteration of imputation using &lt;code&gt;mice&lt;/code&gt; (van Buuren &amp;amp; Groothuis-Oudshoorn, 2011). This is not an appropriate method to analyze missing data but for the purpose of the example I am just using the one complete data. For the sake of this example, let‚Äôs analyze the effect of math efficacy on later math achievement.&lt;/p&gt;
&lt;div id=&#34;loading-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

dat &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/meghapsimatrix/R_practice/master/Data/HSLS09_complete.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-numerators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Numerators&lt;/h2&gt;
&lt;p&gt;Here I am getting the numerators of the IPW, the marginal densities. I have regressed math_efficacy on just the intercept and used &lt;code&gt;dnorm&lt;/code&gt; function to extract the densities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the numerator
mod_num &amp;lt;- lm(math_efficacy ~ 1, data = dat)

num &amp;lt;- dnorm(x = dat$math_efficacy, # treatment 
             mean = fitted.values(mod_num), # fitted values
             sd = summary(mod_num)$sigma) # model sigma&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-denominators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Denominators&lt;/h2&gt;
&lt;p&gt;Here I am getting the denominators of the IPW, the conditional densities. I have regressed math_efficacy on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and used &lt;code&gt;dnorm&lt;/code&gt; function to extract the densities. I am not quite sure whether to use the model sigma which divides the sum of errors squared by the degrees of freedom before taking the square root or whether I should just take the standard deviation of the errors. However, with large sample size the difference between the two are negligible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the demonimator
mod_den &amp;lt;- lm(math_efficacy ~ sex + race + language + repeated_grade + IEP + locale + region + SES, data = dat)

den &amp;lt;- dnorm(x = dat$math_efficacy, # treatment variable
             mean = fitted.values(mod_den), # fitted values
             sd = summary(mod_den)$sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-ipw&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The IPW&lt;/h2&gt;
&lt;p&gt;Below I calculate the stabilized weights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- dat %&amp;gt;%
  mutate(ipw_s = num/den)

summary(dat$ipw_s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.1398  0.9186  0.9778  1.0001  1.0390  5.9782&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-balance-and-outcome-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking Balance and Outcome Analysis&lt;/h2&gt;
&lt;p&gt;Please check back on my next post :) Short story: For balance, we have to calculate weighted correlations, and for outcome analysis we estimate the expected outcome for each treatment level and compare (Austin, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Bia, M., &amp;amp; Mattei, A. (2008). A Stata package for the estimation of the dose-response function through adjustment for the generalized propensity score. The Stata Journal, 8(3), 354-373.&lt;/p&gt;
&lt;p&gt;Hirano K and Imbens GW. The propensity score with continuous treatments. In: Gelman A and Meng X-L (eds) Applied Bayesian modeling and causal inference from incomplete-data perspectives. Chichester: John Wiley &amp;amp; Sons Ltd, 2004, pp.73‚Äì84.&lt;/p&gt;
&lt;p&gt;Robins JM, Hernan MA and Brumback B. Marginal structural models and causal inference in epidemiology. Epidemiol 2000; 11: 550‚Äì560.&lt;/p&gt;
&lt;p&gt;Rosenbaum, P. R. (1986). Dropping out of high school in the United States: An observational study. Journal of Educational Statistics, 11(3), 207-224.&lt;/p&gt;
&lt;p&gt;Stuart, E. A. (2010). Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics, 25(1), 1.&lt;/p&gt;
&lt;p&gt;van Buuren, S., &amp;amp; Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by
chained equations in r. Journal of Statistical Software, 45 (3), 1‚Äì67. Retrieved from
&lt;a href=&#34;http://www.jstatsoft.org/v45/i03/&#34; class=&#34;uri&#34;&gt;http://www.jstatsoft.org/v45/i03/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van der Wal, W. M., &amp;amp; Geskus, R. B. (2011). Ipw: an R package for inverse probability weighting. J Stat Softw, 43(13), 1-23.&lt;/p&gt;
&lt;p&gt;Zhu, Y., Coffman, D. L., &amp;amp; Ghosh, D. (2015). A boosting algorithm for estimating generalized propensity scores with continuous treatments. Journal of causal inference, 3(1), 25-40.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
