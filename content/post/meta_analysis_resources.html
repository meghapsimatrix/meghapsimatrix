---
title: "Meta-Analysis Resources"
author: "Megha Joshi"
date: 2021-01-22
categories: ["R"]
tags: ["meta-analysis"]
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="r-packages" class="section level1">
<h1>R Packages</h1>
<ul>
<li><a href="https://wviechtb.github.io/metafor/index.html">metafor</a></li>
</ul>
<div id="rve-small-sample-corrections" class="section level2">
<h2>RVE small sample corrections</h2>
<ul>
<li><a href="https://cran.r-project.org/web/packages/robumeta/index.html">robumeta</a></li>
<li><a href="https://www.jepusto.com/software/clubsandwich/">clubSandwich</a></li>
<li><a href="https://meghapsimatrix.github.io/wildmeta/">wildmeta</a></li>
</ul>
</div>
<div id="forest-plots" class="section level2">
<h2>Forest plots</h2>
<ul>
<li><a href="https://cran.r-project.org/web/packages/metaviz/vignettes/metaviz.html">metaviz</a></li>
</ul>
</div>
<div id="machine-learning" class="section level2">
<h2>Machine learning</h2>
<ul>
<li><a href="https://cran.r-project.org/web/packages/metaforest/vignettes/Introduction_to_metaforest.html">metaforest</a></li>
</ul>
</div>
</div>
<div id="papers" class="section level1">
<h1>Papers</h1>
<div id="effect-sizes" class="section level2">
<h2>Effect sizes</h2>
<p><a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/j.1750-8606.2008.00060.x?casa_token=hrW46KhivQ4AAAAA%3AUXWiARQ2gw2nDby94QWoZZaLyn-8bkPd-RgEzEBZL-6_LOA4Ca0_bjoCqJ6MZjGS35Dt5elGE6_v">Hedges, L. V. (2008). What are effect sizes and why do we need them?. Child Development Perspectives, 2(3), 167-171.</a></p>
<p>“Effect sizes are quantitative indexes of the relations between variables found in research studies.They can provide a broadly understandable summary of research findings that can be used to compare different studies or summarize results across studies. Unlike statistical significance (p values), effect sizes represent strength of relationships without regard to sample size.Three families of effect sizes are widely used: the standardized mean difference family, the standardized regression coefficient family, and the odds ratio family.”</p>
</div>
<div id="assumption-underlying-pooling-effect-sizes" class="section level2">
<h2>Assumption underlying pooling effect sizes</h2>
<p><a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssa.12275">Rice, K., Higgins, J. P., &amp; Lumley, T. (2018). A re-evaluation of fixed effect (s) meta-analysis. JR Stat Soc Ser A Stat Soc, 181(1), 205-27.</a></p>
<p>“Meta-analysis is a common tool for synthesizing results of multiple studies. Among methods for performing meta-analysis, the approach known as ‘fixed-effects’ or ‘inverse-variance weighting’ is popular and widely-used. A common interpretation of this method is that it assumes the underlying effects in contributing studies are identical, and for this reason it is sometimes dismissed by practitioners. However, other interpretations of fixed-effects analyses do not make this assumption, yet appear to be little known in the literature. In this paper, we review these alternative interpretations, describing both their strengths and limitations. We also describe how heterogeneity of the underlying effects can be addressed, with the same minimal assumptions, through either testing or meta-regression. Recommendations for the practice of meta-analysis are given; it is hoped that these will foster more direct connection of the questions meta-analysts wish to answer with the statistical methods they choose.”</p>
<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667312/">Higgins, J. P., Thompson, S. G., &amp; Spiegelhalter, D. J. (2009). A re‐evaluation of random‐effects meta‐analysis. Journal of the Royal Statistical Society: Series A (Statistics in Society), 172(1), 137-159.</a></p>
<p>“Meta-analysis in the presence of unexplained heterogeneity is frequently under- taken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpre- tation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without prob- lems, including computational intensity and sensitivity to a priori judgments. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of ‘set shifting’ ability in people with eating disorders.”</p>
</div>
<div id="measures-of-heterogeneity" class="section level2">
<h2>Measures of heterogeneity</h2>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/12111919/">Higgins, J. P., &amp; Thompson, S. G. (2002). Quantifying heterogeneity in a meta‐analysis. Statistics in medicine, 21(11), 1539-1558.</a></p>
<p>“The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.”</p>
</div>
<div id="dependent-effect-sizes" class="section level2">
<h2>Dependent Effect Sizes</h2>
<div id="multivariate-meta-analysis" class="section level3">
<h3>Multivariate meta-analysis</h3>
<p>Becker, B. J. (2000). Multivariate meta-analysis. In Handbook of applied multivariate statistics and mathematical modeling (pp. 499-525). Academic Press.</p>
<p>Chapter from a book discussing multivariate meta-analysis.</p>
</div>
<div id="robust-variance-estimation" class="section level3">
<h3>Robust variance estimation</h3>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/26056092/">Hedges, L. V., Tipton, E., &amp; Johnson, M. C. (2010). Robust variance estimation in meta‐regression with dependent effect size estimates. Research synthesis methods, 1(1), 39-65.</a></p>
<p>“Conventional meta-analytic techniques rely on the assumption that effect size estimates from different studies are independent and have sampling distributions with known conditional variances. The independence assumption is violated when studies produce several estimates based on the same individuals or there are clusters of studies that are not independent (such as those carried out by the same investigator or laboratory). This paper provides an estimator of the covariance matrix of meta-regression coefficients that are applicable when there are clusters of internally correlated estimates. It makes no assumptions about the specific form of the sampling distributions of the effect sizes, nor does it require knowledge of the covariance structure of the dependent estimates. Moreover, this paper demonstrates that the meta-regression coefficients are consistent and asymptotically normally distributed and that the robust variance estimator is valid even when the covariates are random. The theory is asymptotic in the number of studies, but simulations suggest that the theory may yield accurate results with as few as 20–40 studies.”</p>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/24773356/">Tipton, E. (2015). Small sample adjustments for robust variance estimation with meta-regression. Psychological methods, 20(3), 375.</a></p>
<p>“Although primary studies often report multiple outcomes, the covariances between these outcomes are rarely reported. This leads to difficulties when combining studies in a meta-analysis. This problem was recently addressed with the introduction of robust variance estimation. This new method enables the estimation of meta-regression models with dependent effect sizes, even when the dependence structure is unknown. Although robust variance estimation has been shown to perform well when the number of studies in the meta-analysis is large, previous simulation studies suggest that the associated tests often have Type I error rates that are much larger than nominal. In this article, I introduce 6 estimators with better small sample properties and study the effectiveness of these estimators via 2 simulation studies. The results of these simulations suggest that the best estimator involves correcting both the residuals and degrees of freedom used in the robust variance estimator. These studies also suggest that the degrees of freedom depend on not only the number of studies but also the type of covariates in the meta-regression. The fact that the degrees of freedom can be small, even when the number of studies is large, suggests that these small-sample corrections should be used more generally. I conclude with an example comparing the results of a meta-regression with robust variance estimation with the results from the corrected estimator.”</p>
<p><a href="https://journals.sagepub.com/doi/10.3102/1076998615606099">Tipton, E., &amp; Pustejovsky, J. E. (2015). Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression. Journal of Educational and Behavioral Statistics, 40(6), 604-634.</a><br />
“Meta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling’s T2 distribution, is level-a and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large- sample F-test.”</p>
</div>
<div id="rve-expanded-working-models" class="section level3">
<h3>RVE expanded working models</h3>
<p><a href="https://osf.io/preprints/metaarxiv/vyfcj/">Pustejovsky, J. E., &amp; Tipton, E. (2020). Meta-Analysis with Robust Variance Estimation: Expanding the Range of Working Models.</a></p>
<p>“In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the nature of the dependence is unknown.RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing On flexible tools from multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the ‘metafor’ and ‘clubSandwich’ packages for R)and illustrate the approaching meta-analysis of randomized trials examining the effects of brief alcohol interventions for adolescents and young adults.”</p>
</div>
</div>
<div id="model-diagnostics" class="section level2">
<h2>Model Diagnostics</h2>
<p><a href="https://onlinelibrary.wiley.com/doi/10.1002/jrsm.11">Viechtbauer, W., &amp; Cheung, M. W. L. (2010). Outlier and influence diagnostics for meta‐analysis. Research synthesis methods, 1(2), 112-125.</a></p>
<p>“The presence of outliers and influential cases may affect the validity and robustness of the conclusions from a meta-analysis. While researchers generally agree that it is necessary to examine outlier and influential case diagnostics when conducting a meta-analysis, limited studies have addressed how to obtain such diagnostic measures in the context of a meta-analysis. The present paper extends standard diagnostic procedures developed for linear regression analyses to the meta-analytic fixed- and random/mixed-effects models. Three examples are used to illustrate the usefulness of these procedures in various research settings. Issues related to these diagnostic procedures in meta-analysis are also discussed.”</p>
</div>
<div id="publication-bias" class="section level2">
<h2>Publication Bias</h2>
<p><a href="https://osf.io/preprints/metaarxiv/vqp8u/">Rodgers, M. A., &amp; Pustejovsky, J. E. (2020). Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes. Psychological methods.</a></p>
<p>“Selective reporting of results based on their statistical significance threatens the validity of meta-analytic findings. A variety of techniques for detecting selective reporting, publication bias, or small-study effects are available and are routinely used in research syntheses. Most such techniques are univariate, in that they assume that each study contributes a single, in-dependent effect size estimate to the meta-analysis. In practice, however, studies often con-tribute multiple, statistically dependent effect size estimates, such as for multiple measures of a common outcome construct. Many methods are available for meta-analyzing dependent effect sizes, but methods for investigating selective reporting while also handling effect size dependencies require further investigation. Using Monte Carlo simulations, we evaluate three available univariate tests for small-study effects or selective reporting, including the Trim &amp; Fill test, Egger’s regression test, and a likelihood ratio test from a three-parameter selection model (3PSM), when dependence is ignored or handled using ad-hoc techniques. We also examine two variants of Egger’s regression test that incorporate robust variance estimation (RVE)or multi-level meta-analysis (MLMA) to handle dependence. Simulation results demonstrate that ignoring dependence inflates Type I error rates for all univariate tests. Variants of Egger’s Regression maintain Type I error rates when dependent effect sizes are sampled or handled using RVE or MLMA. The 3PSM likelihood ratio test does not fully control Type I error rates.With the exception of the 3PSM, all methods have limited power to detect selection bias except under strong selection for statistically significant effects.”</p>
<p><a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12440">Mathur, M. B., &amp; VanderWeele, T. J. (2020). Sensitivity analysis for publication bias in meta‐analyses. Journal of the Royal Statistical Society: Series C (Applied Statistics), 69(5), 1091-1119.</a></p>
<p>“We propose sensitivity analyses for publication bias in meta‐analyses. We consider a publication process such that ‘statistically significant’ results are more likely to be published than negative or “non‐significant” results by an unknown ratio, η. Our proposed methods also accommodate some plausible forms of selection based on a study’s standard error. Using inverse probability weighting and robust estimation that accommodates non‐normal population effects, small meta‐analyses, and clustering, we develop sensitivity analyses that enable statements such as ‘For publication bias to shift the observed point estimate to the null, “significant” results would need to be at least 30 fold more likely to be published than negative or “non‐significant” results’. Comparable statements can be made regarding shifting to a chosen non‐null value or shifting the confidence interval. To aid interpretation, we describe empirical benchmarks for plausible values of η across disciplines. We show that a worst‐case meta‐analytic point estimate for maximal publication bias under the selection model can be obtained simply by conducting a standard meta‐analysis of only the negative and ‘non‐significant’ studies; this method sometimes indicates that no amount of such publication bias could ‘explain away’ the results. We illustrate the proposed methods by using real meta‐analyses and provide an R package: PublicationBias."</p>
</div>
</div>
